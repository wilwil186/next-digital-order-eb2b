{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8c28a3",
   "metadata": {},
   "source": [
    "# Modelado — Próximo pedido DIGITAL (100% PySpark)\n",
    "\n",
    "Este cuaderno implementa los **siguientes pasos** del EDA:\n",
    "- Construcción de **features de comportamiento** (RFM simplificado y agregados transaccionales).\n",
    "- Preparación de un **dataset a nivel cliente-mes** con la **etiqueta `próximo pedido = DIGITAL`**.\n",
    "- Entrenamiento de un modelo supervisado sencillo (**Logistic Regression** de Spark ML).\n",
    "- **Evaluación** con métricas estándar (AUC ROC, AUC PR, Accuracy, Precision, Recall, F1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96ef5f",
   "metadata": {},
   "source": [
    "## 1) Sesión Spark y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71564bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"modelado-proximo-pedido-digital\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "DATA_DIR = \"dataset/dataset\"     # cambia si tu ruta es distinta\n",
    "TEST_START_YM = \"2024-01\"        # train < 2024-01 ; test >= 2024-01\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b53b55",
   "metadata": {},
   "source": [
    "## 2) Carga y preparación mínima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ad44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(DATA_DIR)\n",
    "\n",
    "expected = [\n",
    "    \"cliente_id\",\"pais_cd\",\"region_comercial_txt\",\"agencia_id\",\"ruta_id\",\n",
    "    \"tipo_cliente_cd\",\"madurez_digital_cd\",\"estrellas_txt\",\"frecuencia_visitas_cd\",\n",
    "    \"fecha_pedido_dt\",\"canal_pedido_cd\",\"facturacion_usd_val\",\n",
    "    \"materiales_distintos_val\",\"cajas_fisicas\"\n",
    "]\n",
    "present = [c for c in expected if c in df.columns]\n",
    "print(\"Columnas presentes:\", present)\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
    "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\") == \"DIGITAL\", 1).otherwise(0))\n",
    ")\n",
    "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef6cba",
   "metadata": {},
   "source": [
    "## 3) Etiqueta: **¿el próximo pedido del cliente es DIGITAL?**\n",
    "\n",
    "Tomamos el **último pedido de cada cliente-mes** y usamos si su **próximo pedido** es DIGITAL como etiqueta (`label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc())\n",
    "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc())\n",
    "\n",
    "orders = (\n",
    "    df\n",
    "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
    "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
    "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
    "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
    "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
    ")\n",
    "\n",
    "last_in_month = (\n",
    "    orders\n",
    "    .filter(F.col(\"rn_month_desc\")==1)\n",
    "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
    "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
    "            F.col(\"next_is_digital\").alias(\"label\"))\n",
    ")\n",
    "last_in_month.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a97a6d",
   "metadata": {},
   "source": [
    "## 4) Features a nivel **cliente-mes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd34216",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_agg = (\n",
    "    df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"n_orders\"),\n",
    "          F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
    "          F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
    "          F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
    "          F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
    "          F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
    "          F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
    "          F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
    "          F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
    "          F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
    "          F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
    "          F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
    "      )\n",
    ")\n",
    "\n",
    "from pyspark.sql import Window\n",
    "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
    "\n",
    "ds = (monthly_agg\n",
    "      .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
    "      .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
    "      .filter(F.col(\"label\").isNotNull()))\n",
    "\n",
    "ds.select(\"cliente_id\",\"ym\",\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_cajas\",\"recency_days_last\",\"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c2dc8",
   "metadata": {},
   "source": [
    "## 5) Split temporal: train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4799339",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.filter(F.col(\"ym\") < F.lit(TEST_START_YM))\n",
    "test  = ds.filter(F.col(\"ym\") >= F.lit(TEST_START_YM))\n",
    "\n",
    "print(\"Train rows:\", train.count(), \" | Test rows:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a67845",
   "metadata": {},
   "source": [
    "## 6) Pipeline de modelado (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\"]\n",
    "cat_cols = [\"tipo_cliente_cd\",\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"region_comercial_txt\"]\n",
    "\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=[c + \"_imp\" for c in num_cols])\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[c + \"_idx\"], outputCols=[c + \"_oh\"]) for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=[c + \"_imp\" for c in num_cols] + [c + \"_oh\" for c in cat_cols],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50, regParam=0.01)\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler, lr])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "print(model.stages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624ea99",
   "metadata": {},
   "source": [
    "## 7) Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.transform(test).cache()\n",
    "pred_test.select(\"cliente_id\",\"ym\",\"label\",\"probability\",\"prediction\").show(5, truncate=False)\n",
    "\n",
    "e_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "e_pr  = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "\n",
    "auc = e_auc.evaluate(pred_test)\n",
    "aupr = e_pr.evaluate(pred_test)\n",
    "\n",
    "# Métricas derivadas con umbral por defecto 0.5\n",
    "cm = pred_test.groupBy(\"label\",\"prediction\").count().toPandas()\n",
    "\n",
    "tp = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "tn = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "fp = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "fn = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "\n",
    "accuracy  = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "precision = tp / max(tp + fp, 1)\n",
    "recall    = tp / max(tp + fn, 1)\n",
    "f1        = (2 * precision * recall) / max(precision + recall, 1e-9)\n",
    "\n",
    "print(f\"AUC ROC : {auc:.4f}\")\n",
    "print(f\"AUC PR  : {aupr:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall  : {recall:.4f}\")\n",
    "print(f\"F1      : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nMatriz de confusión (label x prediction):\")\n",
    "print(cm.sort_values(['label','prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a5ee5",
   "metadata": {},
   "source": [
    "## 8) Próximos pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf80342",
   "metadata": {},
   "source": [
    "- Ajustar **umbral de decisión** según la curva PR (si se prioriza Precision o Recall).\n",
    "- Más **ingeniería de variables** (ventanas móviles 3–6 meses, crecimiento, estacionalidad).\n",
    "- **Backtesting** temporal con múltiples cortes.\n",
    "- Probar modelos **árbol/ensemble** (RandomForest, GBT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b51f1",
   "metadata": {},
   "source": [
    "## 9) Cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
