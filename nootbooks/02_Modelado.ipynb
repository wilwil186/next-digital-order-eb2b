{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "97b18259",
      "metadata": {},
      "source": [
        "# Modelado ‚Äî Pr√≥ximo pedido **DIGITAL** (v3.2, PySpark LR + Desbalance + Backtesting + PR@k)\n",
        "\n",
        "**Fecha:** 2025-09-16  \n",
        "**Autor:** Wilson Eduardo Jerez Hern√°ndez\n",
        "\n",
        "## ¬øQu√© vas a ver?\n",
        "- **Objetivo:** predecir si el **pr√≥ximo pedido** de un cliente ser√° **DIGITAL**.\n",
        "- **M√©tricas clave de negocio:** F1 y **PR@k** (+ **Lift@k**) para campa√±as.\n",
        "- **Estrategia:** Regresi√≥n Log√≠stica en PySpark con manejo de **desbalance** y **validaci√≥n temporal** (*backtesting*).\n",
        "- **Explicabilidad:** coeficientes del modelo (LR).\n",
        "- **Buenas pr√°cticas:** evitar **fuga de informaci√≥n** (*data leakage*), cortar linajes con *checkpoint*, y configurar Spark para estabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2eee0d5",
      "metadata": {},
      "source": [
        "## üß† Conceptos clave (mini‚Äëteor√≠a)\n",
        "\n",
        "### Backtesting (validaci√≥n temporal)\n",
        "- **Qu√©:** Simula el uso del modelo en distintos momentos del pasado.  \n",
        "- **C√≥mo:** Entrenas con datos **anteriores** a un corte temporal (p. ej., `2023-10`) y pruebas en lo **posterior**. Repites con varios cortes.  \n",
        "- **Por qu√©:** Los datos cambian con el tiempo. El backtesting verifica **robustez temporal** y evita sesgos de una sola partici√≥n.\n",
        "\n",
        "### AUC ROC vs AUC PR\n",
        "- **AUC ROC:** mide la **capacidad de separar** positivos/negativos en todos los umbrales (TPR vs FPR). 1.0 = perfecto, 0.5 = azar.  \n",
        "- **AUC PR:** m√°s informativa con **clases desbalanceadas**; resume **Precisi√≥n vs Recall**.  \n",
        "- **Regla pr√°ctica:** reporta ambas; conf√≠a m√°s en **AUC PR** y **PR@k** cuando la clase positiva es rara.\n",
        "\n",
        "### PR@k y Lift@k (enfoque de campa√±a)\n",
        "- **PR@k:** Precisi√≥n en el **top k%** clientes seg√∫n score.  \n",
        "- **Recall@k:** Porcentaje de positivos capturados en ese top k%.  \n",
        "- **Lift@k = PR@k / tasa_base:** qu√© tanto **multiplicas** a tirar al azar. √ötil para estimar impacto de campa√±as.\n",
        "\n",
        "### Desbalance de clases\n",
        "- **Problema:** pocos 1s (DIGITAL). Si optimizas accuracy, el modelo ‚Äúaprende‚Äù a predecir casi todo 0.  \n",
        "- **Soluciones:** `weightCol` (clase positiva pesa m√°s), **oversampling** de 1s, **undersampling** de 0s.  \n",
        "- **M√©trica:** usa **AUC PR**, **F1** y **PR@k/Lift@k**.\n",
        "\n",
        "### Evitar *Data Leakage*\n",
        "- **Regla de oro:** todo c√°lculo para el punto de predicci√≥n debe usar **solo el pasado** del cliente.  \n",
        "- En este notebook, la etiqueta por cliente‚Äëmes se construye mirando el **siguiente** pedido y las *features* usan ventanas temporales correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67963984",
      "metadata": {},
      "source": [
        "## 1) Sesi√≥n Spark\n",
        "\n",
        "- **Qu√©:** crear una sesi√≥n estable en `local[*]` y configurar memoria/serializaci√≥n.\n",
        "- **Por qu√©:** datasets medianos/grandes y *pipelines* con OHE requieren evitar OOM y DAGs largos.\n",
        "- **C√≥mo:** activar **Kryo**, subir `driver.memory`, ajustar `shuffle.partitions` y usar `checkpoint`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "061dd802",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/09/17 17:56:43 WARN Utils: Your hostname, debian, resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface wlo1)\n",
            "25/09/17 17:56:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/09/17 17:56:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "import math\n",
        "\n",
        "try:\n",
        "    spark.stop()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"modelado-proximo-pedido-digital-v3.2-lr-imbalance-backtest\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "    .config(\"spark.driver.memory\", \"12g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "    .config(\"spark.kryoserializer.buffer\", \"32m\")\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
        "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark_chk\")\n",
        "\n",
        "DATA_DIR = \"../dataset/dataset\"   # <-- ajusta si tu ruta cambia\n",
        "DEFAULT_TEST_START_YM = \"2024-01\"  # corte por defecto\n",
        "BACKTEST_SPLITS = [\"2023-08\", \"2023-10\", \"2023-12\", \"2024-01\"]  # puedes editar\n",
        "print(\"Spark version:\", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8c2589",
      "metadata": {},
      "source": [
        "## 2) Carga y preparaci√≥n m√≠nima\n",
        "- **Qu√©:** leer parquet, derivar columnas temporales y binaria `is_digital`.\n",
        "- **Por qu√©:** normalizamos el tiempo por **mes** (`ym`) para construir features y etiquetas por cliente‚Äëmes.\n",
        "- **C√≥mo:** `trunc(fecha, 'month')`, `date_format`, y `when(... == 'DIGITAL', 1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f8efb784",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+-------+---------------+----------+\n",
            "|cliente_id|fecha_pedido_dt    |ym     |canal_pedido_cd|is_digital|\n",
            "+----------+-------------------+-------+---------------+----------+\n",
            "|C089085   |2023-05-16 19:00:00|2023-05|VENDEDOR       |0         |\n",
            "|C073952   |2023-10-06 19:00:00|2023-10|VENDEDOR       |0         |\n",
            "|C101443   |2023-01-04 19:00:00|2023-01|DIGITAL        |1         |\n",
            "|C055939   |2024-01-12 19:00:00|2024-01|DIGITAL        |1         |\n",
            "|C088826   |2023-09-30 19:00:00|2023-09|DIGITAL        |1         |\n",
            "+----------+-------------------+-------+---------------+----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.parquet(DATA_DIR)\n",
        "\n",
        "df = (df\n",
        "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
        "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\")==\"DIGITAL\", 1).otherwise(0))\n",
        ")\n",
        "\n",
        "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1b5657",
      "metadata": {},
      "source": [
        "## 3) Etiqueta por **cliente‚Äëmes** (sin fuga)\n",
        "\n",
        "- **Qu√©:** para cada cliente y mes, tomar el **√∫ltimo pedido** del mes y etiquetar con si el **siguiente pedido** del cliente fue DIGITAL (`label` ‚àà {0,1}).\n",
        "- **Por qu√©:** queremos predecir el **pr√≥ximo** comportamiento, no el del mismo mes; as√≠ **evitamos fuga**.\n",
        "- **C√≥mo:** ventanas por cliente (`lag/lead`) y *row_number()* descendente dentro del mes para quedarse con el √∫ltimo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "42110063",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 4:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+-------+-----------------+-----+\n",
            "|cliente_id|month_first|ym     |recency_days_last|label|\n",
            "+----------+-----------+-------+-----------------+-----+\n",
            "|C000009   |2023-01-01 |2023-01|NULL             |0    |\n",
            "|C000009   |2023-08-01 |2023-08|213              |0    |\n",
            "|C000009   |2023-11-01 |2023-11|4                |0    |\n",
            "|C000009   |2023-12-01 |2023-12|8                |1    |\n",
            "|C000009   |2024-01-01 |2024-01|28               |1    |\n",
            "+----------+-----------+-------+-----------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "all_cols = df.columns\n",
        "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc(),\n",
        "                                                          F.hash(*[F.col(c) for c in all_cols]).asc())\n",
        "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc(),\n",
        "                                                                             F.hash(*[F.col(c) for c in all_cols]).desc())\n",
        "\n",
        "orders = (df\n",
        "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
        "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
        "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
        "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
        "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
        ")\n",
        "\n",
        "last_in_month = (orders\n",
        "    .filter(F.col(\"rn_month_desc\")==1)\n",
        "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
        "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
        "            F.col(\"next_is_digital\").alias(\"label\"))\n",
        ")\n",
        "last_in_month.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d587afc5",
      "metadata": {},
      "source": [
        "## 4) Features (RFM + rolling + *priors* + ciclo de vida)\n",
        "\n",
        "- **Qu√©:** agregados por cliente‚Äëmes (actividad y valor), *rolling* 3m, crecimiento, se√±ales de **contexto** por regi√≥n/tipo, y **ciclo de vida**.\n",
        "- **Por qu√©:** combinamos se√±ales **propias** del cliente, **tendencias** recientes y **entorno** del segmento para mejorar ranking.\n",
        "- **C√≥mo:** `groupBy` + ventanas temporales por cliente; *lags* por regi√≥n/tipo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09744755",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/09/17 17:56:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "[Stage 30:====================================================> (196 + 4) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows ds: 1022849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "monthly_agg = (df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"n_orders\"),\n",
        "        F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
        "        F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
        "        F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
        "        F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
        "        F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
        "        F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
        "        F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
        "        F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
        "        F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
        "        F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
        "        F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
        "    )\n",
        ")\n",
        "\n",
        "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
        "first_month = (monthly_agg\n",
        "               .withColumn(\"first_month\", F.first(\"month_first\", ignorenulls=True).over(w_client_month))\n",
        "               .select(\"cliente_id\",\"first_month\").distinct())\n",
        "monthly_agg = (monthly_agg\n",
        "               .join(first_month, on=\"cliente_id\", how=\"left\")\n",
        "               .withColumn(\"months_since_first\", F.floor(F.months_between(\"month_first\", \"first_month\"))))\n",
        "\n",
        "region_month = (df.groupBy(\"region_comercial_txt\",\"month_first\").agg(F.avg(\"is_digital\").alias(\"region_digital_ratio\")))\n",
        "region_month = region_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "w_region = Window.partitionBy(\"region_comercial_txt\").orderBy(F.col(\"month_first\").asc())\n",
        "region_month = (region_month\n",
        "                .withColumn(\"region_digital_ratio_lag1\", F.lag(\"region_digital_ratio\", 1).over(w_region))\n",
        "                .select(\"region_comercial_txt\",\"ym\",\"region_digital_ratio_lag1\"))\n",
        "\n",
        "tipo_month = (df.groupBy(\"tipo_cliente_cd\",\"month_first\").agg(F.avg(\"is_digital\").alias(\"tipo_digital_ratio\")))\n",
        "tipo_month = tipo_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "w_tipo = Window.partitionBy(\"tipo_cliente_cd\").orderBy(F.col(\"month_first\").asc())\n",
        "tipo_month = (tipo_month\n",
        "              .withColumn(\"tipo_digital_ratio_lag1\", F.lag(\"tipo_digital_ratio\", 1).over(w_tipo))\n",
        "              .select(\"tipo_cliente_cd\",\"ym\",\"tipo_digital_ratio_lag1\"))\n",
        "\n",
        "w_roll3 = w_client_month.rowsBetween(-3, -1)\n",
        "ds = (monthly_agg\n",
        "    .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
        "    .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
        "    .withColumn(\"n_orders_3m\", F.sum(\"n_orders\").over(w_roll3))\n",
        "    .withColumn(\"digital_ratio_3m\", F.avg(\"digital_ratio\").over(w_roll3))\n",
        "    .withColumn(\"sum_fact_3m\", F.sum(\"sum_fact\").over(w_roll3))\n",
        "    .withColumn(\"growth_digital_ratio\", F.col(\"digital_ratio\") - F.col(\"lag1_digital_ratio\"))\n",
        "    .join(region_month, on=[\"region_comercial_txt\",\"ym\"], how=\"left\")\n",
        "    .join(tipo_month, on=[\"tipo_cliente_cd\",\"ym\"], how=\"left\")\n",
        "    .filter(F.col(\"label\").isNotNull())\n",
        ")\n",
        "ds.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "print(\"Rows ds:\", ds.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a62014",
      "metadata": {},
      "source": [
        "## 5) Utilidades de evaluaci√≥n y desbalance\n",
        "\n",
        "- **Qu√©:** funciones para `weightCol`, **over/under‚Äësampling**, **PR@k/Recall@k/Lift@k** y **barrido de umbral** para F1.\n",
        "- **Por qu√©:** el **desbalance** requiere t√©cnicas espec√≠ficas y medir **ranking** (PR@k) adem√°s de m√©tricas globales.\n",
        "- **C√≥mo:** contar clases para pesos; remuestrear positivos/negativos; ordenar por score para top‚Äëk; barrer thresholds 0..1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9233f553",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_class_weights(df_train, label_col=\"label\"):\n",
        "    pos = df_train.filter(F.col(label_col)==1).count()\n",
        "    neg = df_train.filter(F.col(label_col)==0).count()\n",
        "    ratio = (neg / float(max(pos,1))) if pos else 1.0\n",
        "    return df_train.withColumn(\"weight\", F.when(F.col(label_col)==1, F.lit(ratio)).otherwise(F.lit(1.0)))\n",
        "\n",
        "def oversample_minority(df_train, label_col=\"label\", target_ratio=0.5):\n",
        "    # target_ratio = proporci√≥n de positivos deseada (ej: 0.5 => 1:1)\n",
        "    counts = df_train.groupBy(label_col).count().collect()\n",
        "    cnt = {int(r[label_col]): r['count'] for r in counts}\n",
        "    pos, neg = cnt.get(1,0), cnt.get(0,0)\n",
        "    if pos==0 or neg==0:\n",
        "        return df_train\n",
        "    current_ratio = pos / float(pos+neg)\n",
        "    if current_ratio >= target_ratio:\n",
        "        return df_train\n",
        "    desired_pos = int(math.ceil(target_ratio * (pos+neg) / (1 - target_ratio)))  # algebra inversa\n",
        "    add_pos = max(desired_pos - pos, 0)\n",
        "    if add_pos <= 0:\n",
        "        return df_train\n",
        "    # sampling con reemplazo para positivos\n",
        "    frac = add_pos / float(pos)\n",
        "    df_pos = df_train.filter(F.col(label_col)==1)\n",
        "    df_pos_extra = df_pos.sample(withReplacement=True, fraction=frac, seed=42)\n",
        "    return df_train.unionByName(df_pos_extra)\n",
        "\n",
        "def undersample_majority(df_train, label_col=\"label\", target_ratio=0.5):\n",
        "    counts = df_train.groupBy(label_col).count().collect()\n",
        "    cnt = {int(r[label_col]): r['count'] for r in counts}\n",
        "    pos, neg = cnt.get(1,0), cnt.get(0,0)\n",
        "    if pos==0 or neg==0:\n",
        "        return df_train\n",
        "    desired_neg = int((pos * (1 - target_ratio)) / max(target_ratio, 1e-6))\n",
        "    keep_neg = max(min(desired_neg, neg), 1)\n",
        "    frac = keep_neg / float(neg)\n",
        "    df_neg = df_train.filter(F.col(label_col)==0).sample(withReplacement=False, fraction=frac, seed=42)\n",
        "    df_pos = df_train.filter(F.col(label_col)==1)\n",
        "    return df_pos.unionByName(df_neg)\n",
        "\n",
        "def precision_recall_at_k(pred_df, k=0.1, label_col=\"label\", score_col=\"p1\"):\n",
        "    total = pred_df.count()\n",
        "    k_n = max(int(total * k), 1)\n",
        "    topk = pred_df.orderBy(F.col(score_col).desc()).limit(k_n)\n",
        "    tp = topk.filter(F.col(label_col)==1).count()\n",
        "    positives = pred_df.filter(F.col(label_col)==1).count()\n",
        "    precision = tp / float(k_n)\n",
        "    recall = tp / float(max(positives,1))\n",
        "    # Lift@k = precision@k / base_rate\n",
        "    base_rate = positives / float(max(total,1))\n",
        "    lift = precision / float(max(base_rate,1e-9))\n",
        "    return precision, recall, lift\n",
        "\n",
        "def best_threshold_for_f1(pred_df, label_col=\"label\", score_col=\"p1\", grid_size=101):\n",
        "    # Eval√∫a F1 en thresholds uniformes [0,1]\n",
        "    best = (0.5, 0.0, 0.0, 0.0)  # thr, precision, recall, f1\n",
        "    for i in range(grid_size):\n",
        "        thr = i / float(grid_size-1)\n",
        "        pred = pred_df.withColumn(\"pred\", F.when(F.col(score_col)>=thr, F.lit(1)).otherwise(F.lit(0)))\n",
        "        cm = pred.groupBy(label_col, \"pred\").count().toPandas()\n",
        "        tp = int(cm[(cm[label_col]==1) & (cm[\"pred\"]==1)][\"count\"].sum())\n",
        "        tn = int(cm[(cm[label_col]==0) & (cm[\"pred\"]==0)][\"count\"].sum())\n",
        "        fp = int(cm[(cm[label_col]==0) & (cm[\"pred\"]==1)][\"count\"].sum())\n",
        "        fn = int(cm[(cm[label_col]==1) & (cm[\"pred\"]==0)][\"count\"].sum())\n",
        "        precision = tp / float(max(tp+fp,1))\n",
        "        recall    = tp / float(max(tp+fn,1))\n",
        "        f1 = (2*precision*recall) / float(max(precision+recall,1e-9))\n",
        "        if f1 > best[3]:\n",
        "            best = (thr, precision, recall, f1)\n",
        "    return best"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ff4a67",
      "metadata": {},
      "source": [
        "## 6) Pipeline LR + grids compactos\n",
        "\n",
        "- **Qu√©:** Pipeline con imputaci√≥n, indexaci√≥n + OHE (baja cardinalidad), *assembler* y **Regresi√≥n Log√≠stica** con `weightCol`.\n",
        "- **Por qu√©:** LR es **estable, explicable y r√°pida**; soporta pesos de clase y funciona bien como **ranker** base.\n",
        "- **C√≥mo:** definimos `num_cols` y `cat_low`, armamos `ParamGrid` y usamos `TrainValidationSplit` con **AUC PR**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "401980d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = [\n",
        "    \"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\n",
        "    \"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\",\n",
        "    \"n_orders_3m\",\"digital_ratio_3m\",\"sum_fact_3m\",\"growth_digital_ratio\",\n",
        "    \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\"\n",
        "]\n",
        "\n",
        "cat_low = [\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"tipo_cliente_cd\"]\n",
        "\n",
        "imputer = Imputer(inputCols=num_cols, outputCols=[c+\"_imp\" for c in num_cols])\n",
        "idxs = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in cat_low]\n",
        "ohe = OneHotEncoder(inputCols=[c+\"_idx\" for c in cat_low], outputCols=[c+\"_oh\" for c in cat_low])\n",
        "feats = [c+\"_imp\" for c in num_cols] + [c+\"_oh\" for c in cat_low]\n",
        "asm = VectorAssembler(inputCols=feats, outputCol=\"features_lr\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features_lr\", labelCol=\"label\", weightCol=\"weight\",\n",
        "                        maxIter=80, regParam=0.01, elasticNetParam=0.0)\n",
        "\n",
        "pipe_lr = Pipeline(stages=[imputer] + idxs + [ohe, asm, lr])\n",
        "\n",
        "grid_lr = (ParamGridBuilder()\n",
        "           .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
        "           .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "           .build())\n",
        "\n",
        "e_pr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
        "tvs_lr = TrainValidationSplit(estimator=pipe_lr, estimatorParamMaps=grid_lr, evaluator=e_pr, trainRatio=0.85, parallelism=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc572939",
      "metadata": {},
      "source": [
        "## 7) Entrenador con estrategia de desbalance\n",
        "\n",
        "- **Qu√©:** funci√≥n que aplica `weights`, `oversample` o `undersample` antes de entrenar.\n",
        "- **Por qu√©:** ajustar la **severidad** del desbalance puede mejorar F1/PR@k.\n",
        "- **C√≥mo:** materializamos con `checkpoint(eager=True)` para **cortar el DAG** y evitar fallos de memoria/linaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ae0be0c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lr_with_imbalance(train_df, strategy=\"weights\"):\n",
        "    df_tr = train_df\n",
        "    if strategy == \"weights\":\n",
        "        df_tr = apply_class_weights(df_tr)\n",
        "    elif strategy == \"oversample\":\n",
        "        df_tr = oversample_minority(df_tr)\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "    elif strategy == \"undersample\":\n",
        "        df_tr = undersample_majority(df_tr)\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "    else:\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "\n",
        "    df_tr = df_tr.checkpoint(eager=True)\n",
        "    model = tvs_lr.fit(df_tr)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2f70cb",
      "metadata": {},
      "source": [
        "## 8) Evaluaci√≥n (AUC ROC/PR, F1 √≥ptimo, PR@k/Recall@k/Lift@k) ‚Äî **Qu√© / Por qu√© / C√≥mo**\n",
        "\n",
        "- **Qu√©:** calcular m√©tricas globales (**AUC ROC/PR**) y de campa√±a (**PR@k/Lift@k**), y buscar **umbral** que maximiza F1.\n",
        "- **Por qu√©:** F1 balancea precisi√≥n/recall; PR@k y Lift@k permiten **dimensionar campa√±as** y ROI.\n",
        "- **C√≥mo:** extraer `p1 = probability[1]`, evaluar con `BinaryClassificationEvaluator`, `precision_recall_at_k` y `best_threshold_for_f1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cf6dfb2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_predictions(pred, label_col=\"label\"):\n",
        "    pred = pred.withColumn(\"p1\", vector_to_array(\"probability\")[1]).cache()\n",
        "    e_auc  = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    e_aupr = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
        "    auc  = e_auc.evaluate(pred)\n",
        "    aupr = e_aupr.evaluate(pred)\n",
        "    thr, p_thr, r_thr, f1_thr = best_threshold_for_f1(pred.select(label_col, \"p1\"))\n",
        "    p5, r5, l5 = precision_recall_at_k(pred.select(label_col, \"p1\"), k=0.05)\n",
        "    p10, r10, l10 = precision_recall_at_k(pred.select(label_col, \"p1\"), k=0.10)\n",
        "    metrics = {\n",
        "        \"auc_roc\": auc, \"auc_pr\": aupr,\n",
        "        \"best_threshold\": thr, \"precision_at_best_thr\": p_thr, \"recall_at_best_thr\": r_thr, \"f1_at_best_thr\": f1_thr,\n",
        "        \"precision@5%\": p5, \"recall@5%\": r5, \"lift@5%\": l5,\n",
        "        \"precision@10%\": p10, \"recall@10%\": r10, \"lift@10%\": l10,\n",
        "    }\n",
        "    return pred, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b2f0f3",
      "metadata": {},
      "source": [
        "## 9) Backtesting\n",
        "\n",
        "- **Qu√©:** evaluar el modelo en **m√∫ltiples cortes** temporales (`BACKTEST_SPLITS`).  \n",
        "- **Por qu√©:** medir **robustez temporal** y comparar rendimiento en diferentes per√≠odos.  \n",
        "- **C√≥mo:** `train = ds[ym < corte]`, `test = ds[ym >= corte]`; entrenar, predecir y registrar m√©tricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "337a46b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-08 ===\n",
            "Train rows: 361852 | Test rows: 660997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-08', 'auc_roc': 0.62331, 'auc_pr': 0.499691, 'best_threshold': 0.0, 'precision_at_best_thr': 0.371971, 'recall_at_best_thr': 1.0, 'f1_at_best_thr': 0.542244, 'precision@5%': 0.579594, 'recall@5%': 0.077906, 'lift@5%': 1.558168, 'precision@10%': 0.579025, 'recall@10%': 0.155662, 'lift@10%': 1.556639}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-10 ===\n",
            "Train rows: 465637 | Test rows: 557212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-10', 'auc_roc': 0.619602, 'auc_pr': 0.47347, 'best_threshold': 0.38, 'precision_at_best_thr': 0.446656, 'recall_at_best_thr': 0.632661, 'f1_at_best_thr': 0.523631, 'precision@5%': 0.549174, 'recall@5%': 0.077891, 'lift@5%': 1.557848, 'precision@10%': 0.547783, 'recall@10%': 0.155389, 'lift@10%': 1.5539}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-12 ===\n",
            "Train rows: 569153 | Test rows: 453696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-12', 'auc_roc': 0.614559, 'auc_pr': 0.437458, 'best_threshold': 0.38, 'precision_at_best_thr': 0.412553, 'recall_at_best_thr': 0.632214, 'f1_at_best_thr': 0.499292, 'precision@5%': 0.510007, 'recall@5%': 0.07825, 'lift@5%': 1.565051, 'precision@10%': 0.506668, 'recall@10%': 0.155478, 'lift@10%': 1.554804}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2024-01 ===\n",
            "Train rows: 621769 | Test rows: 401080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2024-01', 'auc_roc': 0.611897, 'auc_pr': 0.413518, 'best_threshold': 0.38, 'precision_at_best_thr': 0.389729, 'recall_at_best_thr': 0.632464, 'f1_at_best_thr': 0.482276, 'precision@5%': 0.482946, 'recall@5%': 0.078485, 'lift@5%': 1.569692, 'precision@10%': 0.479081, 'recall@10%': 0.155713, 'lift@10%': 1.557131}\n"
          ]
        }
      ],
      "source": [
        "def run_backtests(ds, test_splits, imbalance_strategy=\"weights\"):\n",
        "    results = []\n",
        "    for cut in test_splits:\n",
        "        print(f\"\\n=== Backtest corte TEST_START_YM = {cut} ===\")\n",
        "        train = ds.filter(F.col(\"ym\") < F.lit(cut))\n",
        "        test  = ds.filter(F.col(\"ym\") >= F.lit(cut))\n",
        "        print(\"Train rows:\", train.count(), \"| Test rows:\", test.count())\n",
        "        train = train.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
        "        _ = train.count()\n",
        "        model = train_lr_with_imbalance(train, strategy=imbalance_strategy)\n",
        "        pred = model.transform(test).cache()\n",
        "        pred, met = evaluate_predictions(pred)\n",
        "        met_row = {\n",
        "            \"cut\": cut,\n",
        "            **{k: round(float(v), 6) for k,v in met.items()}\n",
        "        }\n",
        "        results.append(met_row)\n",
        "        print(met_row)\n",
        "    return results\n",
        "\n",
        "results_weights = run_backtests(ds, BACKTEST_SPLITS, imbalance_strategy=\"weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92c3674",
      "metadata": {},
      "source": [
        "## 10) Coeficientes (explicabilidad LR)\n",
        "\n",
        "- **Qu√©:** extraer **coeficientes** e **intercepto** del mejor modelo para el √∫ltimo corte.\n",
        "- **Por qu√©:** LR permite interpretar **direcci√≥n y magnitud** de cada feature (con cautela por colinealidad).\n",
        "- **C√≥mo:** localizar `LogisticRegressionModel` en el `PipelineModel`, leer `coefficients` y mapear nombres desde el *metadata* de `features_lr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aaeccb60",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: 0.2634366115978072\n",
            "N¬∫ coef: 30\n",
            "                          feature      coef  abs_coef\n",
            "18                   avg_fact_imp  0.816328  0.816328\n",
            "16         lag1_digital_ratio_imp -0.765888  0.765888\n",
            "2      madurez_digital_cd_oh_ALTA  0.000000  0.000000\n",
            "3     frecuencia_visitas_cd_oh_LM  0.000000  0.000000\n",
            "0      madurez_digital_cd_oh_BAJA  0.000000  0.000000\n",
            "1     madurez_digital_cd_oh_MEDIA  0.000000  0.000000\n",
            "6    frecuencia_visitas_cd_oh_LMV  0.000000  0.000000\n",
            "7                   pais_cd_oh_GT  0.000000  0.000000\n",
            "8                   pais_cd_oh_EC  0.000000  0.000000\n",
            "9                   pais_cd_oh_PE  0.000000  0.000000\n",
            "10                  pais_cd_oh_SV  0.000000  0.000000\n",
            "11      tipo_cliente_cd_oh_TIENDA  0.000000  0.000000\n",
            "4      frecuencia_visitas_cd_oh_L  0.000000  0.000000\n",
            "5    frecuencia_visitas_cd_oh_LMI  0.000000  0.000000\n",
            "13   tipo_cliente_cd_oh_MAYORISTA  0.000000  0.000000\n",
            "12  tipo_cliente_cd_oh_MINIMARKET  0.000000  0.000000\n",
            "15              digital_ratio_imp  0.000000  0.000000\n",
            "14                   n_orders_imp  0.000000  0.000000\n",
            "17                   sum_fact_imp  0.000000  0.000000\n",
            "19                  sum_cajas_imp  0.000000  0.000000\n",
            "20                  avg_cajas_imp  0.000000  0.000000\n",
            "21               avg_mat_dist_imp  0.000000  0.000000\n",
            "22          recency_days_last_imp  0.000000  0.000000\n",
            "23                n_orders_3m_imp  0.000000  0.000000\n",
            "24           digital_ratio_3m_imp  0.000000  0.000000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "\n",
        "# Entrena una vez para extraer coeficientes sobre el √∫ltimo corte\n",
        "cut = BACKTEST_SPLITS[-1] if BACKTEST_SPLITS else DEFAULT_TEST_START_YM\n",
        "train = ds.filter(F.col(\"ym\") < F.lit(cut))\n",
        "\n",
        "model = train_lr_with_imbalance(train, strategy=\"weights\")\n",
        "best  = model.bestModel  # <- PipelineModel\n",
        "\n",
        "# 1) Ubicar la etapa correcta (modelo ya entrenado)\n",
        "stage_types = [type(s).__name__ for s in best.stages]\n",
        "lr_stage = next((s for s in best.stages if isinstance(s, LogisticRegressionModel)), None)\n",
        "if lr_stage is None:\n",
        "    raise ValueError(f\"No encontr√© LogisticRegressionModel en stages: {stage_types}\")\n",
        "\n",
        "# 2) Coeficientes e intercepto\n",
        "coef = lr_stage.coefficients.toArray()\n",
        "intercept = lr_stage.intercept\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"N¬∫ coef:\", len(coef))\n",
        "\n",
        "# 3) (Opcional) Mapear cada coeficiente al nombre de feature\n",
        "#    Tomamos el metadata de 'features_lr' para obtener los nombres\n",
        "tmp = best.transform(train.limit(1))\n",
        "meta = tmp.schema[\"features_lr\"].metadata.get(\"ml_attr\", {})\n",
        "attrs = []\n",
        "for k in (\"binary\", \"numeric\"):  # OHE -> binary ; num√©ricas -> numeric\n",
        "    if \"attrs\" in meta and k in meta[\"attrs\"]:\n",
        "        attrs += meta[\"attrs\"][k]\n",
        "\n",
        "feat_names = [a[\"name\"] for a in attrs]\n",
        "# puede haber un peque√±o desfase si hay atributos sin nombre; recortamos al m√≠nimo\n",
        "n = min(len(feat_names), len(coef))\n",
        "feat_names = feat_names[:n]\n",
        "coef = coef[:n]\n",
        "\n",
        "# Top-25 por magnitud\n",
        "import pandas as pd\n",
        "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coef})\n",
        "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
        "print(coef_df.sort_values(\"abs_coef\", ascending=False).head(25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7459af8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Cierre ordenado de la sesi√≥n\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
