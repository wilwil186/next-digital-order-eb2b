{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelado — Próximo pedido **DIGITAL** (v3.2, PySpark LR + Desbalance + Backtesting + PR@k)\n",
        "\n",
        "**Fecha de generación del cuaderno:** 2025-09-16\n",
        "\n",
        "**Objetivo:** predecir si el **próximo pedido** de un cliente será **DIGITAL**, optimizando **F1** y **PR@k** (métricas útiles para campañas).  \n",
        "**Enfoque:**\n",
        "- Modelo central: **Regresión Logística (LR)** (estable, explicable, rápido).  \n",
        "- **Estrategias de desbalance** sobre el *train*: `weightCol`, **oversampling**, **undersampling**.  \n",
        "- **Backtesting** en múltiples cortes temporales.  \n",
        "- **Tuning de umbral** para maximizar F1 y cálculo de **Precision/Recall@k** y **Lift@k**.\n",
        "\n",
        "> Nota: Dejamos *ganchos* para integrar LightGBM/XGBoost si tu entorno lo permite. En Spark 4.0.1 puede requerir dependencias específicas; este notebook funciona 100% con PySpark estándar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Sesión Spark (config estable)\n",
        "- Ejecuta en `local[*]`.\n",
        "- Activa **Kryo** y sube memoria para evitar OOM.\n",
        "- Ajusta `DATA_DIR` según tu ruta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/09/17 07:06:38 WARN Utils: Your hostname, debian, resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface wlo1)\n",
            "25/09/17 07:06:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/09/17 07:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "import math\n",
        "\n",
        "try:\n",
        "    spark.stop()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"modelado-proximo-pedido-digital-v3.2-lr-imbalance-backtest\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "    .config(\"spark.driver.memory\", \"12g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "    .config(\"spark.kryoserializer.buffer\", \"32m\")\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
        "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark_chk\")\n",
        "\n",
        "DATA_DIR = \"dataset/dataset\"   # <-- ajusta si tu ruta cambia\n",
        "DEFAULT_TEST_START_YM = \"2024-01\"  # corte por defecto\n",
        "BACKTEST_SPLITS = [\"2023-08\", \"2023-10\", \"2023-12\", \"2024-01\"]  # puedes editar\n",
        "print(\"Spark version:\", spark.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Carga y preparación mínima\n",
        "- Derivamos `month_first`, `ym`, `is_digital`.\n",
        "- Mantén tipos simples y nombres consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+-------+---------------+----------+\n",
            "|cliente_id|fecha_pedido_dt    |ym     |canal_pedido_cd|is_digital|\n",
            "+----------+-------------------+-------+---------------+----------+\n",
            "|C089085   |2023-05-16 19:00:00|2023-05|VENDEDOR       |0         |\n",
            "|C073952   |2023-10-06 19:00:00|2023-10|VENDEDOR       |0         |\n",
            "|C101443   |2023-01-04 19:00:00|2023-01|DIGITAL        |1         |\n",
            "|C055939   |2024-01-12 19:00:00|2024-01|DIGITAL        |1         |\n",
            "|C088826   |2023-09-30 19:00:00|2023-09|DIGITAL        |1         |\n",
            "+----------+-------------------+-------+---------------+----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.parquet(DATA_DIR)\n",
        "\n",
        "df = (df\n",
        "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
        "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\")==\"DIGITAL\", 1).otherwise(0))\n",
        ")\n",
        "\n",
        "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Etiqueta por **cliente-mes** (sin fuga)\n",
        "- Tomamos el **último pedido** del mes por cliente.\n",
        "- `label` = si el **próximo pedido** del cliente es DIGITAL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 4:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+-------+-----------------+-----+\n",
            "|cliente_id|month_first|ym     |recency_days_last|label|\n",
            "+----------+-----------+-------+-----------------+-----+\n",
            "|C000009   |2023-01-01 |2023-01|NULL             |0    |\n",
            "|C000009   |2023-08-01 |2023-08|213              |0    |\n",
            "|C000009   |2023-11-01 |2023-11|4                |0    |\n",
            "|C000009   |2023-12-01 |2023-12|8                |1    |\n",
            "|C000009   |2024-01-01 |2024-01|28               |1    |\n",
            "+----------+-----------+-------+-----------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "all_cols = df.columns\n",
        "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc(),\n",
        "                                                          F.hash(*[F.col(c) for c in all_cols]).asc())\n",
        "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc(),\n",
        "                                                                             F.hash(*[F.col(c) for c in all_cols]).desc())\n",
        "\n",
        "orders = (df\n",
        "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
        "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
        "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
        "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
        "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
        ")\n",
        "\n",
        "last_in_month = (orders\n",
        "    .filter(F.col(\"rn_month_desc\")==1)\n",
        "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
        "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
        "            F.col(\"next_is_digital\").alias(\"label\"))\n",
        ")\n",
        "last_in_month.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Features (RFM + rolling + priors + ciclo de vida)\n",
        "- Agregados por cliente-mes, ratios por segmento, rolling 3m, crecimiento, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/09/17 07:06:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "[Stage 30:=================================================>   (188 + 12) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows ds: 1022849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "monthly_agg = (df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"n_orders\"),\n",
        "        F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
        "        F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
        "        F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
        "        F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
        "        F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
        "        F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
        "        F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
        "        F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
        "        F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
        "        F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
        "        F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
        "    )\n",
        ")\n",
        "\n",
        "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
        "first_month = (monthly_agg\n",
        "               .withColumn(\"first_month\", F.first(\"month_first\", ignorenulls=True).over(w_client_month))\n",
        "               .select(\"cliente_id\",\"first_month\").distinct())\n",
        "monthly_agg = (monthly_agg\n",
        "               .join(first_month, on=\"cliente_id\", how=\"left\")\n",
        "               .withColumn(\"months_since_first\", F.floor(F.months_between(\"month_first\", \"first_month\"))))\n",
        "\n",
        "region_month = (df.groupBy(\"region_comercial_txt\",\"month_first\").agg(F.avg(\"is_digital\").alias(\"region_digital_ratio\")))\n",
        "region_month = region_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "w_region = Window.partitionBy(\"region_comercial_txt\").orderBy(F.col(\"month_first\").asc())\n",
        "region_month = (region_month\n",
        "                .withColumn(\"region_digital_ratio_lag1\", F.lag(\"region_digital_ratio\", 1).over(w_region))\n",
        "                .select(\"region_comercial_txt\",\"ym\",\"region_digital_ratio_lag1\"))\n",
        "\n",
        "tipo_month = (df.groupBy(\"tipo_cliente_cd\",\"month_first\").agg(F.avg(\"is_digital\").alias(\"tipo_digital_ratio\")))\n",
        "tipo_month = tipo_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
        "w_tipo = Window.partitionBy(\"tipo_cliente_cd\").orderBy(F.col(\"month_first\").asc())\n",
        "tipo_month = (tipo_month\n",
        "              .withColumn(\"tipo_digital_ratio_lag1\", F.lag(\"tipo_digital_ratio\", 1).over(w_tipo))\n",
        "              .select(\"tipo_cliente_cd\",\"ym\",\"tipo_digital_ratio_lag1\"))\n",
        "\n",
        "w_roll3 = w_client_month.rowsBetween(-3, -1)\n",
        "ds = (monthly_agg\n",
        "    .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
        "    .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
        "    .withColumn(\"n_orders_3m\", F.sum(\"n_orders\").over(w_roll3))\n",
        "    .withColumn(\"digital_ratio_3m\", F.avg(\"digital_ratio\").over(w_roll3))\n",
        "    .withColumn(\"sum_fact_3m\", F.sum(\"sum_fact\").over(w_roll3))\n",
        "    .withColumn(\"growth_digital_ratio\", F.col(\"digital_ratio\") - F.col(\"lag1_digital_ratio\"))\n",
        "    .join(region_month, on=[\"region_comercial_txt\",\"ym\"], how=\"left\")\n",
        "    .join(tipo_month, on=[\"tipo_cliente_cd\",\"ym\"], how=\"left\")\n",
        "    .filter(F.col(\"label\").isNotNull())\n",
        ")\n",
        "ds.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "print(\"Rows ds:\", ds.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Utilidades: desbalance, PR@k/Recall@k, F1 vs threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_class_weights(df_train, label_col=\"label\"):\n",
        "    pos = df_train.filter(F.col(label_col)==1).count()\n",
        "    neg = df_train.filter(F.col(label_col)==0).count()\n",
        "    ratio = (neg / float(max(pos,1))) if pos else 1.0\n",
        "    return df_train.withColumn(\"weight\", F.when(F.col(label_col)==1, F.lit(ratio)).otherwise(F.lit(1.0)))\n",
        "\n",
        "def oversample_minority(df_train, label_col=\"label\", target_ratio=0.5):\n",
        "    # target_ratio = proporción de positivos deseada (ej: 0.5 => 1:1)\n",
        "    counts = df_train.groupBy(label_col).count().collect()\n",
        "    cnt = {int(r[label_col]): r['count'] for r in counts}\n",
        "    pos, neg = cnt.get(1,0), cnt.get(0,0)\n",
        "    if pos==0 or neg==0:\n",
        "        return df_train\n",
        "    current_ratio = pos / float(pos+neg)\n",
        "    if current_ratio >= target_ratio:\n",
        "        return df_train\n",
        "    desired_pos = int(math.ceil(target_ratio * (pos+neg) / (1 - target_ratio)))  # algebra inversa\n",
        "    add_pos = max(desired_pos - pos, 0)\n",
        "    if add_pos <= 0:\n",
        "        return df_train\n",
        "    # sampling con reemplazo para positivos\n",
        "    frac = add_pos / float(pos)\n",
        "    df_pos = df_train.filter(F.col(label_col)==1)\n",
        "    df_pos_extra = df_pos.sample(withReplacement=True, fraction=frac, seed=42)\n",
        "    return df_train.unionByName(df_pos_extra)\n",
        "\n",
        "def undersample_majority(df_train, label_col=\"label\", target_ratio=0.5):\n",
        "    counts = df_train.groupBy(label_col).count().collect()\n",
        "    cnt = {int(r[label_col]): r['count'] for r in counts}\n",
        "    pos, neg = cnt.get(1,0), cnt.get(0,0)\n",
        "    if pos==0 or neg==0:\n",
        "        return df_train\n",
        "    desired_neg = int((pos * (1 - target_ratio)) / max(target_ratio, 1e-6))\n",
        "    keep_neg = max(min(desired_neg, neg), 1)\n",
        "    frac = keep_neg / float(neg)\n",
        "    df_neg = df_train.filter(F.col(label_col)==0).sample(withReplacement=False, fraction=frac, seed=42)\n",
        "    df_pos = df_train.filter(F.col(label_col)==1)\n",
        "    return df_pos.unionByName(df_neg)\n",
        "\n",
        "def precision_recall_at_k(pred_df, k=0.1, label_col=\"label\", score_col=\"p1\"):\n",
        "    total = pred_df.count()\n",
        "    k_n = max(int(total * k), 1)\n",
        "    topk = pred_df.orderBy(F.col(score_col).desc()).limit(k_n)\n",
        "    tp = topk.filter(F.col(label_col)==1).count()\n",
        "    positives = pred_df.filter(F.col(label_col)==1).count()\n",
        "    precision = tp / float(k_n)\n",
        "    recall = tp / float(max(positives,1))\n",
        "    # Lift@k = precision@k / base_rate\n",
        "    base_rate = positives / float(max(total,1))\n",
        "    lift = precision / float(max(base_rate,1e-9))\n",
        "    return precision, recall, lift\n",
        "\n",
        "def best_threshold_for_f1(pred_df, label_col=\"label\", score_col=\"p1\", grid_size=101):\n",
        "    # Evalúa F1 en thresholds uniformes [0,1]\n",
        "    best = (0.5, 0.0, 0.0, 0.0)  # thr, precision, recall, f1\n",
        "    for i in range(grid_size):\n",
        "        thr = i / float(grid_size-1)\n",
        "        pred = pred_df.withColumn(\"pred\", F.when(F.col(score_col)>=thr, F.lit(1)).otherwise(F.lit(0)))\n",
        "        cm = pred.groupBy(label_col, \"pred\").count().toPandas()\n",
        "        tp = int(cm[(cm[label_col]==1) & (cm[\"pred\"]==1)][\"count\"].sum())\n",
        "        tn = int(cm[(cm[label_col]==0) & (cm[\"pred\"]==0)][\"count\"].sum())\n",
        "        fp = int(cm[(cm[label_col]==0) & (cm[\"pred\"]==1)][\"count\"].sum())\n",
        "        fn = int(cm[(cm[label_col]==1) & (cm[\"pred\"]==0)][\"count\"].sum())\n",
        "        precision = tp / float(max(tp+fp,1))\n",
        "        recall    = tp / float(max(tp+fn,1))\n",
        "        f1 = (2*precision*recall) / float(max(precision+recall,1e-9))\n",
        "        if f1 > best[3]:\n",
        "            best = (thr, precision, recall, f1)\n",
        "    return best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Pipeline LR + grids compactos\n",
        "- OHE solo para **baja cardinalidad**.\n",
        "- Imputación de numéricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = [\n",
        "    \"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\n",
        "    \"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\",\n",
        "    \"n_orders_3m\",\"digital_ratio_3m\",\"sum_fact_3m\",\"growth_digital_ratio\",\n",
        "    \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\"\n",
        "]\n",
        "\n",
        "cat_low = [\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"tipo_cliente_cd\"]\n",
        "\n",
        "imputer = Imputer(inputCols=num_cols, outputCols=[c+\"_imp\" for c in num_cols])\n",
        "idxs = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in cat_low]\n",
        "ohe = OneHotEncoder(inputCols=[c+\"_idx\" for c in cat_low], outputCols=[c+\"_oh\" for c in cat_low])\n",
        "feats = [c+\"_imp\" for c in num_cols] + [c+\"_oh\" for c in cat_low]\n",
        "asm = VectorAssembler(inputCols=feats, outputCol=\"features_lr\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features_lr\", labelCol=\"label\", weightCol=\"weight\",\n",
        "                        maxIter=80, regParam=0.01, elasticNetParam=0.0)\n",
        "\n",
        "pipe_lr = Pipeline(stages=[imputer] + idxs + [ohe, asm, lr])\n",
        "\n",
        "grid_lr = (ParamGridBuilder()\n",
        "           .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
        "           .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "           .build())\n",
        "\n",
        "e_pr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
        "tvs_lr = TrainValidationSplit(estimator=pipe_lr, estimatorParamMaps=grid_lr, evaluator=e_pr, trainRatio=0.85, parallelism=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Entrenador con estrategia de desbalance\n",
        "Parámetro `imbalance_strategy` puede ser: `\"weights\"` (default), `\"oversample\"`, `\"undersample\"`, `\"none\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lr_with_imbalance(train_df, strategy=\"weights\"):\n",
        "    df_tr = train_df\n",
        "    if strategy == \"weights\":\n",
        "        df_tr = apply_class_weights(df_tr)\n",
        "    elif strategy == \"oversample\":\n",
        "        df_tr = oversample_minority(df_tr)\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "    elif strategy == \"undersample\":\n",
        "        df_tr = undersample_majority(df_tr)\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "    else:\n",
        "        df_tr = df_tr.withColumn(\"weight\", F.lit(1.0))\n",
        "\n",
        "    df_tr = df_tr.checkpoint(eager=True)\n",
        "    model = tvs_lr.fit(df_tr)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Evaluación: AUC PR, F1 (best-threshold), PR@k/Recall@k/Lift@k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_predictions(pred, label_col=\"label\"):\n",
        "    pred = pred.withColumn(\"p1\", vector_to_array(\"probability\")[1]).cache()\n",
        "    e_auc  = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    e_aupr = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
        "    auc  = e_auc.evaluate(pred)\n",
        "    aupr = e_aupr.evaluate(pred)\n",
        "    thr, p_thr, r_thr, f1_thr = best_threshold_for_f1(pred.select(label_col, \"p1\"))\n",
        "    p5, r5, l5 = precision_recall_at_k(pred.select(label_col, \"p1\"), k=0.05)\n",
        "    p10, r10, l10 = precision_recall_at_k(pred.select(label_col, \"p1\"), k=0.10)\n",
        "    metrics = {\n",
        "        \"auc_roc\": auc, \"auc_pr\": aupr,\n",
        "        \"best_threshold\": thr, \"precision_at_best_thr\": p_thr, \"recall_at_best_thr\": r_thr, \"f1_at_best_thr\": f1_thr,\n",
        "        \"precision@5%\": p5, \"recall@5%\": r5, \"lift@5%\": l5,\n",
        "        \"precision@10%\": p10, \"recall@10%\": r10, \"lift@10%\": l10,\n",
        "    }\n",
        "    return pred, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Backtesting: múltiples cortes `TEST_START_YM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-08 ===\n",
            "Train rows: 361852 | Test rows: 660997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-08', 'auc_roc': 0.62331, 'auc_pr': 0.499691, 'best_threshold': 0.0, 'precision_at_best_thr': 0.371971, 'recall_at_best_thr': 1.0, 'f1_at_best_thr': 0.542244, 'precision@5%': 0.579594, 'recall@5%': 0.077906, 'lift@5%': 1.558168, 'precision@10%': 0.579025, 'recall@10%': 0.155662, 'lift@10%': 1.556639}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-10 ===\n",
            "Train rows: 465637 | Test rows: 557212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-10', 'auc_roc': 0.619602, 'auc_pr': 0.47347, 'best_threshold': 0.38, 'precision_at_best_thr': 0.446656, 'recall_at_best_thr': 0.632661, 'f1_at_best_thr': 0.523631, 'precision@5%': 0.549174, 'recall@5%': 0.077891, 'lift@5%': 1.557848, 'precision@10%': 0.547783, 'recall@10%': 0.155389, 'lift@10%': 1.5539}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2023-12 ===\n",
            "Train rows: 569153 | Test rows: 453696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2023-12', 'auc_roc': 0.614559, 'auc_pr': 0.437458, 'best_threshold': 0.38, 'precision_at_best_thr': 0.412553, 'recall_at_best_thr': 0.632214, 'f1_at_best_thr': 0.499292, 'precision@5%': 0.510007, 'recall@5%': 0.07825, 'lift@5%': 1.565051, 'precision@10%': 0.506668, 'recall@10%': 0.155478, 'lift@10%': 1.554804}\n",
            "\n",
            "=== Backtest corte TEST_START_YM = 2024-01 ===\n",
            "Train rows: 621769 | Test rows: 401080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut': '2024-01', 'auc_roc': 0.611897, 'auc_pr': 0.413518, 'best_threshold': 0.38, 'precision_at_best_thr': 0.389729, 'recall_at_best_thr': 0.632464, 'f1_at_best_thr': 0.482276, 'precision@5%': 0.482946, 'recall@5%': 0.078485, 'lift@5%': 1.569692, 'precision@10%': 0.479081, 'recall@10%': 0.155713, 'lift@10%': 1.557131}\n"
          ]
        }
      ],
      "source": [
        "def run_backtests(ds, test_splits, imbalance_strategy=\"weights\"):\n",
        "    results = []\n",
        "    for cut in test_splits:\n",
        "        print(f\"\\n=== Backtest corte TEST_START_YM = {cut} ===\")\n",
        "        train = ds.filter(F.col(\"ym\") < F.lit(cut))\n",
        "        test  = ds.filter(F.col(\"ym\") >= F.lit(cut))\n",
        "        print(\"Train rows:\", train.count(), \"| Test rows:\", test.count())\n",
        "        train = train.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
        "        _ = train.count()\n",
        "        model = train_lr_with_imbalance(train, strategy=imbalance_strategy)\n",
        "        pred = model.transform(test).cache()\n",
        "        pred, met = evaluate_predictions(pred)\n",
        "        met_row = {\n",
        "            \"cut\": cut,\n",
        "            **{k: round(float(v), 6) for k,v in met.items()}\n",
        "        }\n",
        "        results.append(met_row)\n",
        "        print(met_row)\n",
        "    return results\n",
        "\n",
        "results_weights = run_backtests(ds, BACKTEST_SPLITS, imbalance_strategy=\"weights\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Coeficientes (explicabilidad LR)\n",
        "- Muestra los coeficientes del mejor modelo del último corte (como referencia)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1b876d0d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: 0.2634366115978087\n",
            "Nº coef: 30\n",
            "                          feature      coef  abs_coef\n",
            "18                   avg_fact_imp  0.816328  0.816328\n",
            "16         lag1_digital_ratio_imp -0.765888  0.765888\n",
            "2      madurez_digital_cd_oh_ALTA  0.000000  0.000000\n",
            "3     frecuencia_visitas_cd_oh_LM  0.000000  0.000000\n",
            "0      madurez_digital_cd_oh_BAJA  0.000000  0.000000\n",
            "1     madurez_digital_cd_oh_MEDIA  0.000000  0.000000\n",
            "6    frecuencia_visitas_cd_oh_LMV  0.000000  0.000000\n",
            "7                   pais_cd_oh_GT  0.000000  0.000000\n",
            "8                   pais_cd_oh_EC  0.000000  0.000000\n",
            "9                   pais_cd_oh_PE  0.000000  0.000000\n",
            "10                  pais_cd_oh_SV  0.000000  0.000000\n",
            "11      tipo_cliente_cd_oh_TIENDA  0.000000  0.000000\n",
            "4      frecuencia_visitas_cd_oh_L  0.000000  0.000000\n",
            "5    frecuencia_visitas_cd_oh_LMI  0.000000  0.000000\n",
            "13   tipo_cliente_cd_oh_MAYORISTA  0.000000  0.000000\n",
            "12  tipo_cliente_cd_oh_MINIMARKET  0.000000  0.000000\n",
            "15              digital_ratio_imp  0.000000  0.000000\n",
            "14                   n_orders_imp  0.000000  0.000000\n",
            "17                   sum_fact_imp  0.000000  0.000000\n",
            "19                  sum_cajas_imp  0.000000  0.000000\n",
            "20                  avg_cajas_imp  0.000000  0.000000\n",
            "21               avg_mat_dist_imp  0.000000  0.000000\n",
            "22          recency_days_last_imp  0.000000  0.000000\n",
            "23                n_orders_3m_imp  0.000000  0.000000\n",
            "24           digital_ratio_3m_imp  0.000000  0.000000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "\n",
        "# Entrena una vez para extraer coeficientes sobre el último corte\n",
        "cut = BACKTEST_SPLITS[-1] if BACKTEST_SPLITS else DEFAULT_TEST_START_YM\n",
        "train = ds.filter(F.col(\"ym\") < F.lit(cut))\n",
        "\n",
        "model = train_lr_with_imbalance(train, strategy=\"weights\")\n",
        "best  = model.bestModel  # <- PipelineModel\n",
        "\n",
        "# 1) Ubicar la etapa correcta (modelo ya entrenado)\n",
        "stage_types = [type(s).__name__ for s in best.stages]\n",
        "lr_stage = next((s for s in best.stages if isinstance(s, LogisticRegressionModel)), None)\n",
        "if lr_stage is None:\n",
        "    raise ValueError(f\"No encontré LogisticRegressionModel en stages: {stage_types}\")\n",
        "\n",
        "# 2) Coeficientes e intercepto\n",
        "coef = lr_stage.coefficients.toArray()\n",
        "intercept = lr_stage.intercept\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"Nº coef:\", len(coef))\n",
        "\n",
        "# 3) (Opcional) Mapear cada coeficiente al nombre de feature\n",
        "#    Tomamos el metadata de 'features_lr' para obtener los nombres\n",
        "tmp = best.transform(train.limit(1))\n",
        "meta = tmp.schema[\"features_lr\"].metadata.get(\"ml_attr\", {})\n",
        "attrs = []\n",
        "for k in (\"binary\", \"numeric\"):  # OHE -> binary ; numéricas -> numeric\n",
        "    if \"attrs\" in meta and k in meta[\"attrs\"]:\n",
        "        attrs += meta[\"attrs\"][k]\n",
        "\n",
        "feat_names = [a[\"name\"] for a in attrs]\n",
        "# puede haber un pequeño desfase si hay atributos sin nombre; recortamos al mínimo\n",
        "n = min(len(feat_names), len(coef))\n",
        "feat_names = feat_names[:n]\n",
        "coef = coef[:n]\n",
        "\n",
        "# Top-25 por magnitud\n",
        "import pandas as pd\n",
        "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coef})\n",
        "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
        "print(coef_df.sort_values(\"abs_coef\", ascending=False).head(25))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c77be843",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Cierre ordenado de la sesión\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Notas\n",
        "- Este cuaderno prioriza ranking (PR@k) y F1, más útiles que accuracy en desbalance.\n",
        "- Puedes reducir `spark.sql.shuffle.partitions` y/o el grid para prototipos rápidos.\n",
        "- Si ves OOM, baja cardinalidad de OHE (quita columnas con demasiadas categorías) o usa *hashing trick*.\n",
        "- Para reporte: exporta la lista de clientes top-k con `p1` (score) y variables clave."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
