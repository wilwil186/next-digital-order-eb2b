{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0007db02",
   "metadata": {},
   "source": [
    "# Modelado — Próximo pedido **DIGITAL** (v3, 100% PySpark + Backtesting & Ensembles)\n",
    "\n",
    "**Objetivo:** predecir si el **próximo pedido** de un cliente será **DIGITAL**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961550a",
   "metadata": {},
   "source": [
    "## 1) Sesión Spark y parámetros\n",
    "\n",
    "- Ejecuta local con todos los cores.\n",
    "\n",
    "- Ajusta `DATA_DIR` si tu dataset está en otra ruta.\n",
    "\n",
    "- `TEST_START_YM` define el **corte temporal** principal.\n",
    "\n",
    "- `BACKTEST_SPLITS` permite evaluar **varios cortes** (opcional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bff58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/16 19:43:20 WARN Utils: Your hostname, debian, resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface wlo1)\n",
      "25/09/16 19:43:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 19:43:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/16 19:43:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/16 19:43:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"modelado-proximo-pedido-digital-v3\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "DATA_DIR = \"dataset/dataset\"      # <--- Ajusta si tu ruta cambia\n",
    "TEST_START_YM = \"2024-01\"         # Entrena con ym < TEST_START_YM | Evalúa con ym >= TEST_START_YM\n",
    "BACKTEST_SPLITS = [\"2023-08\", \"2023-10\", \"2023-12\", \"2024-01\"]  # Opcional\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc3649",
   "metadata": {},
   "source": [
    "## 2) Carga y preparación mínima\n",
    "\n",
    "- Derivamos `month_first`, `ym` y `is_digital`.\n",
    "\n",
    "- Mantenemos **nombres simples** y tipos adecuados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca290975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas presentes: ['cliente_id', 'pais_cd', 'region_comercial_txt', 'agencia_id', 'ruta_id', 'tipo_cliente_cd', 'madurez_digital_cd', 'estrellas_txt', 'frecuencia_visitas_cd', 'fecha_pedido_dt', 'canal_pedido_cd', 'facturacion_usd_val', 'materiales_distintos_val', 'cajas_fisicas']\n",
      "+----------+-------------------+-------+---------------+----------+\n",
      "|cliente_id|fecha_pedido_dt    |ym     |canal_pedido_cd|is_digital|\n",
      "+----------+-------------------+-------+---------------+----------+\n",
      "|C089085   |2023-05-16 19:00:00|2023-05|VENDEDOR       |0         |\n",
      "|C073952   |2023-10-06 19:00:00|2023-10|VENDEDOR       |0         |\n",
      "|C101443   |2023-01-04 19:00:00|2023-01|DIGITAL        |1         |\n",
      "|C055939   |2024-01-12 19:00:00|2024-01|DIGITAL        |1         |\n",
      "|C088826   |2023-09-30 19:00:00|2023-09|DIGITAL        |1         |\n",
      "+----------+-------------------+-------+---------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(DATA_DIR)\n",
    "\n",
    "expected = [\n",
    "    \"cliente_id\",\"pais_cd\",\"region_comercial_txt\",\"agencia_id\",\"ruta_id\",\n",
    "    \"tipo_cliente_cd\",\"madurez_digital_cd\",\"estrellas_txt\",\"frecuencia_visitas_cd\",\n",
    "    \"fecha_pedido_dt\",\"canal_pedido_cd\",\"facturacion_usd_val\",\n",
    "    \"materiales_distintos_val\",\"cajas_fisicas\"\n",
    "]\n",
    "present = [c for c in expected if c in df.columns]\n",
    "print(\"Columnas presentes:\", present)\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
    "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\")==\"DIGITAL\", 1).otherwise(0))\n",
    ")\n",
    "\n",
    "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1afda5",
   "metadata": {},
   "source": [
    "## 3) Etiqueta por **cliente-mes** (determinista, sin fuga)\n",
    "\n",
    "- Orden por `fecha_pedido_dt` (y *tie-break* con hash de todas las columnas).\n",
    "\n",
    "- Para cada mes de un cliente, tomamos el **último pedido** y definimos `label` como si el **próximo pedido** del cliente es DIGITAL.\n",
    "\n",
    "- Agregamos `recency_days_last` (días desde el pedido previo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95849115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+-----------------+-----+\n",
      "|cliente_id|month_first|ym     |recency_days_last|label|\n",
      "+----------+-----------+-------+-----------------+-----+\n",
      "|C000009   |2023-01-01 |2023-01|NULL             |0    |\n",
      "|C000009   |2023-08-01 |2023-08|213              |0    |\n",
      "|C000009   |2023-11-01 |2023-11|4                |0    |\n",
      "|C000009   |2023-12-01 |2023-12|8                |1    |\n",
      "|C000009   |2024-01-01 |2024-01|28               |1    |\n",
      "+----------+-----------+-------+-----------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_cols = df.columns\n",
    "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc(),\n",
    "                                                          F.hash(*[F.col(c) for c in all_cols]).asc())\n",
    "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc(),\n",
    "                                                                             F.hash(*[F.col(c) for c in all_cols]).desc())\n",
    "\n",
    "orders = (df\n",
    "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
    "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
    "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
    "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
    "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
    ")\n",
    "\n",
    "last_in_month = (orders\n",
    "    .filter(F.col(\"rn_month_desc\")==1)\n",
    "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
    "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
    "            F.col(\"next_is_digital\").alias(\"label\"))\n",
    ")\n",
    "\n",
    "last_in_month.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4eec2",
   "metadata": {},
   "source": [
    "## 4) Feature engineering (RFM + rolling + ciclo de vida + priors de segmento)\n",
    "\n",
    "**Señales por cliente-mes**:\n",
    "\n",
    "- Volumen/valor: `n_orders`, `sum_fact`, `avg_fact`, `sum_cajas`, `avg_cajas`, `avg_mat_dist`.\n",
    "\n",
    "- Comportamiento: `digital_ratio` del mes, `lag1_digital_ratio`, **rolling 3m** y `growth_digital_ratio`.\n",
    "\n",
    "- Ciclo de vida: meses desde primer pedido (`months_since_first`).\n",
    "\n",
    "- Priors de segmento (históricos): `region_digital_ratio_lag1`, `tipo_cliente_digital_ratio_lag1`.\n",
    "\n",
    "*(las razones/ratios de segmento se calculan por mes y se desplazan un mes hacia atrás para evitar fuga)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0956987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+-------------+------------------+-----------+-------------------+--------------------+------------------+-------------------------+-----------------------+-----+\n",
      "|cliente_id|ym     |n_orders|digital_ratio|lag1_digital_ratio|n_orders_3m|digital_ratio_3m   |growth_digital_ratio|months_since_first|region_digital_ratio_lag1|tipo_digital_ratio_lag1|label|\n",
      "+----------+-------+--------+-------------+------------------+-----------+-------------------+--------------------+------------------+-------------------------+-----------------------+-----+\n",
      "|C000009   |2023-01|1       |1.0          |NULL              |NULL       |NULL               |NULL                |0                 |0.5092165898617511       |0.5299539170506913     |0    |\n",
      "|C000009   |2023-08|1       |0.0          |1.0               |1          |1.0                |-1.0                |7                 |0.4806050933314829       |0.4780771806893709     |0    |\n",
      "|C000009   |2023-11|3       |0.0          |0.0               |2          |0.5                |0.0                 |10                |0.48336227856894753      |0.47763488241020646    |0    |\n",
      "|C000009   |2023-12|2       |0.5          |0.0               |5          |0.3333333333333333 |0.5                 |11                |0.49121422936240405      |0.48060681047931575    |1    |\n",
      "|C000009   |2024-01|1       |1.0          |0.5               |6          |0.16666666666666666|0.5                 |12                |0.489934293303509        |0.48065699085791025    |1    |\n",
      "+----------+-------+--------+-------------+------------------+-----------+-------------------+--------------------+------------------+-------------------------+-----------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Agregados cliente-mes\n",
    "monthly_agg = (df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_orders\"),\n",
    "        F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
    "        F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
    "        F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
    "        F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
    "        F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
    "        F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
    "        F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
    "        F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
    "        F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
    "        F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
    "        F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ciclo de vida: meses desde primer pedido\n",
    "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
    "first_month = (monthly_agg\n",
    "               .withColumn(\"first_month\", F.first(\"month_first\", ignorenulls=True).over(w_client_month))\n",
    "               .select(\"cliente_id\",\"first_month\").distinct())\n",
    "\n",
    "monthly_agg = monthly_agg.join(first_month, on=\"cliente_id\", how=\"left\")                          .withColumn(\"months_since_first\", F.floor(F.months_between(\"month_first\", \"first_month\")))\n",
    "\n",
    "# Ratios por segmento (mes a mes) y lag para evitar fuga\n",
    "region_month = (df.groupBy(\"region_comercial_txt\",\"month_first\")\n",
    "                  .agg(F.avg(\"is_digital\").alias(\"region_digital_ratio\")))\n",
    "region_month = region_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "w_region = Window.partitionBy(\"region_comercial_txt\").orderBy(F.col(\"month_first\").asc())\n",
    "region_month = region_month.withColumn(\"region_digital_ratio_lag1\", F.lag(\"region_digital_ratio\", 1).over(w_region))                            .select(\"region_comercial_txt\",\"ym\",\"region_digital_ratio_lag1\")\n",
    "\n",
    "tipo_month = (df.groupBy(\"tipo_cliente_cd\",\"month_first\")\n",
    "                .agg(F.avg(\"is_digital\").alias(\"tipo_digital_ratio\")))\n",
    "tipo_month = tipo_month.withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "w_tipo = Window.partitionBy(\"tipo_cliente_cd\").orderBy(F.col(\"month_first\").asc())\n",
    "tipo_month = tipo_month.withColumn(\"tipo_digital_ratio_lag1\", F.lag(\"tipo_digital_ratio\", 1).over(w_tipo))                        .select(\"tipo_cliente_cd\",\"ym\",\"tipo_digital_ratio_lag1\")\n",
    "\n",
    "# Construcción del dataset con label\n",
    "w_roll3 = w_client_month.rowsBetween(-3, -1)  # 3 meses previos\n",
    "ds = (monthly_agg\n",
    "    .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
    "    .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
    "    .withColumn(\"n_orders_3m\", F.sum(\"n_orders\").over(w_roll3))\n",
    "    .withColumn(\"digital_ratio_3m\", F.avg(\"digital_ratio\").over(w_roll3))\n",
    "    .withColumn(\"sum_fact_3m\", F.sum(\"sum_fact\").over(w_roll3))\n",
    "    .withColumn(\"growth_digital_ratio\", F.col(\"digital_ratio\") - F.col(\"lag1_digital_ratio\"))\n",
    "    .join(region_month, on=[\"region_comercial_txt\",\"ym\"], how=\"left\")\n",
    "    .join(tipo_month, on=[\"tipo_cliente_cd\",\"ym\"], how=\"left\")\n",
    "    .filter(F.col(\"label\").isNotNull())\n",
    ")\n",
    "\n",
    "ds.select(\"cliente_id\",\"ym\",\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\n",
    "          \"n_orders_3m\",\"digital_ratio_3m\",\"growth_digital_ratio\",\n",
    "          \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\",\n",
    "          \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d4426",
   "metadata": {},
   "source": [
    "## 5) Split temporal y **balance de clases**\n",
    "\n",
    "- Train: `ym < TEST_START_YM`.\n",
    "\n",
    "- Test:  `ym >= TEST_START_YM`.\n",
    "\n",
    "- Mostramos la **distribución de la etiqueta** y aplicamos **weightCol** cuando hay desbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86757644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 621769  | Test rows: 401080\n",
      "\n",
      "Distribución de la etiqueta:\n",
      "--- train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    0|326236|\n",
      "|    1|295533|\n",
      "+-----+------+\n",
      "\n",
      "--- test ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    0|277680|\n",
      "|    1|123400|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = ds.filter(F.col(\"ym\") < F.lit(TEST_START_YM))\n",
    "test  = ds.filter(F.col(\"ym\") >= F.lit(TEST_START_YM))\n",
    "\n",
    "print(\"Train rows:\", train.count(), \" | Test rows:\", test.count())\n",
    "print(\"\\nDistribución de la etiqueta:\")\n",
    "for name, d in [(\"train\", train), (\"test\", test)]:\n",
    "    print(f\"--- {name} ---\")\n",
    "    d.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38571ab",
   "metadata": {},
   "source": [
    "## 6) Pipeline de modelado (reutilizable)\n",
    "\n",
    "- Imputación para numéricas, Indexación+OneHot para categóricas, `VectorAssembler`.\n",
    "\n",
    "- Modelos soportados: `lr`, `rf`, `gbt`.\n",
    "\n",
    "- **TrainValidationSplit** con *grid* compacto.\n",
    "\n",
    "- **weightCol** para balancear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69db0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\n",
    "            \"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\",\n",
    "            \"n_orders_3m\",\"digital_ratio_3m\",\"sum_fact_3m\",\"growth_digital_ratio\",\n",
    "            \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\"]\n",
    "\n",
    "cat_cols = [\"tipo_cliente_cd\",\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"region_comercial_txt\"]\n",
    "\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=[c + \"_imp\" for c in num_cols])\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[c + \"_idx\"], outputCols=[c + \"_oh\"]) for c in cat_cols]\n",
    "feature_cols = [c + \"_imp\" for c in num_cols] + [c + \"_oh\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "def build_and_fit(model_name: str, train_df):\n",
    "    # Balanceo simple\n",
    "    pos = train_df.filter(F.col(\"label\")==1).count()\n",
    "    neg = train_df.filter(F.col(\"label\")==0).count()\n",
    "    balancing_ratio = neg / float(max(pos, 1)) if pos else 1.0\n",
    "    train_w = train_df.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(balancing_ratio)).otherwise(F.lit(1.0)))\n",
    "\n",
    "    if model_name == \"lr\":\n",
    "        clf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                 maxIter=80, regParam=0.01, elasticNetParam=0.0)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.regParam, [0.0, 0.01, 0.1])\n",
    "                     .addGrid(clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                     .build())\n",
    "    elif model_name == \"rf\":\n",
    "        clf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                     numTrees=200, maxDepth=10, featureSubsetStrategy=\"sqrt\",\n",
    "                                     subsamplingRate=0.8, seed=42)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.numTrees, [150, 200, 300])\n",
    "                     .addGrid(clf.maxDepth, [8, 10, 12])\n",
    "                     .build())\n",
    "    elif model_name == \"gbt\":\n",
    "        clf = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=100, maxDepth=6, stepSize=0.1, seed=42)\n",
    "        # Nota: GBT en Spark no soporta weightCol directamente antes de 3.4; se usa dataset balanceado/estratificado si es necesario.\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.maxIter, [60, 100])\n",
    "                     .addGrid(clf.maxDepth, [5, 6, 8])\n",
    "                     .build())\n",
    "    else:\n",
    "        raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler, clf])\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "\n",
    "    tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                               evaluator=evaluator, trainRatio=0.8, parallelism=2)\n",
    "\n",
    "    tvs_model = tvs.fit(train_w if model_name in (\"lr\",\"rf\") else train_df)  # GBT sin weights\n",
    "    return tvs_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7203fc",
   "metadata": {},
   "source": [
    "## 7) Evaluación: AUC, matriz y métricas derivadas\n",
    "\n",
    "- Reportamos **AUC ROC** y **AUC PR**.\n",
    "\n",
    "- Con el umbral 0.5 para referencia.\n",
    "\n",
    "- Devuelve también las predicciones con la probabilidad `p_digital`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b81a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df, model_name=\"model\"):\n",
    "    pred = model.transform(test_df).withColumn(\"p_digital\", vector_to_array(\"probability\")[1]).cache()\n",
    "    e_auc  = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    e_aupr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    auc = e_auc.evaluate(pred)\n",
    "    aupr = e_aupr.evaluate(pred)\n",
    "\n",
    "    cm = (pred.groupBy(\"label\",\"prediction\").count().toPandas())\n",
    "    tp = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    tn = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "    fp = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    fn = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "\n",
    "    accuracy  = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall    = tp / max(tp + fn, 1)\n",
    "    f1        = (2 * precision * recall) / max(precision + recall, 1e-9)\n",
    "\n",
    "    print(f\"[{model_name}]  AUC ROC: {auc:.4f} | AUC PR: {aupr:.4f} | Acc: {accuracy:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    return pred, {\"auc_roc\": auc, \"auc_pr\": aupr, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cea1e",
   "metadata": {},
   "source": [
    "## 8) Entrenamiento y comparación de modelos (LR, RF, GBT)\n",
    "\n",
    "Seleccionamos el **mejor** por **AUC PR** (prioriza casos positivos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd1ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 19:44:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr]  AUC ROC: 0.6119 | AUC PR: 0.4135 | Acc: 0.5822 | Prec: 0.3897 | Rec: 0.6325 | F1: 0.4823\n",
      "Entrenando modelo: rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 19:48:30 WARN DAGScheduler: Broadcasting large task binary with size 1087.5 KiB\n",
      "25/09/16 19:48:39 WARN DAGScheduler: Broadcasting large task binary with size 1087.5 KiB\n",
      "25/09/16 19:48:54 WARN DAGScheduler: Broadcasting large task binary with size 1832.5 KiB\n",
      "25/09/16 19:49:09 WARN DAGScheduler: Broadcasting large task binary with size 1832.5 KiB\n",
      "25/09/16 19:49:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/09/16 19:49:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/09/16 19:50:10 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/09/16 19:50:31 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/09/16 19:50:54 WARN DAGScheduler: Broadcasting large task binary with size 1541.0 KiB\n",
      "25/09/16 19:51:00 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "25/09/16 19:51:23 WARN DAGScheduler: Broadcasting large task binary with size 1541.0 KiB\n",
      "25/09/16 19:52:00 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/09/16 19:52:10 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "25/09/16 19:53:10 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/09/16 19:53:22 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/09/16 19:53:28 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB\n",
      "25/09/16 19:54:39 WARN DAGScheduler: Broadcasting large task binary with size 1087.5 KiB\n",
      "25/09/16 19:54:57 WARN DAGScheduler: Broadcasting large task binary with size 1333.6 KiB\n",
      "25/09/16 19:55:12 WARN DAGScheduler: Broadcasting large task binary with size 1832.5 KiB\n",
      "25/09/16 19:55:33 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/09/16 19:55:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/09/16 19:56:22 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/09/16 19:56:45 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/09/16 19:57:12 WARN DAGScheduler: Broadcasting large task binary with size 1092.2 KiB\n",
      "25/09/16 19:57:17 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
      "25/09/16 19:57:43 WARN DAGScheduler: Broadcasting large task binary with size 1541.0 KiB\n",
      "25/09/16 19:57:51 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "25/09/16 19:58:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/09/16 19:59:04 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/09/16 19:59:06 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/09/16 19:59:17 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "25/09/16 20:00:24 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/09/16 20:00:38 WARN DAGScheduler: Broadcasting large task binary with size 35.6 MiB\n",
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager.org$apache$spark$storage$BlockManager$RemoteBlockDownloadFileManager$$keepCleaning(BlockManager.scala:2274)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager$$anon$2.run(BlockManager.scala:2240)\n",
      "25/09/16 20:00:53 ERROR Executor: Exception in task 9.0 in stage 2979.0 (TID 105026)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "25/09/16 20:00:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#1531,Executor task launch worker for task 9.0 in stage 2979.0 (TID 105026),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "25/09/16 20:00:54 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@68945b2d rejected from java.util.concurrent.ThreadPoolExecutor@5761e990[Shutting down, pool size = 14, active threads = 12, queued tasks = 0, completed tasks = 105017]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "25/09/16 20:00:54 ERROR TaskSetManager: Task 9 in stage 2979.0 failed 1 times; aborting job\n",
      "25/09/16 20:00:54 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "25/09/16 20:00:54 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@659e7372 rejected from java.util.concurrent.ThreadPoolExecutor@5761e990[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 105028]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 6.0 in stage 2979.0 (TID 105023) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 10.0 in stage 2979.0 (TID 105027) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 3.0 in stage 2979.0 (TID 105020) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 4.0 in stage 2979.0 (TID 105021) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 2.0 in stage 2979.0 (TID 105019) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 1.0 in stage 2979.0 (TID 105018) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 11.0 in stage 2979.0 (TID 105028) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 5.0 in stage 2979.0 (TID 105022) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 7.0 in stage 2979.0 (TID 105024) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 8.0 in stage 2979.0 (TID 105025) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/16 20:00:54 WARN TaskSetManager: Lost task 0.0 in stage 2979.0 (TID 105017) (192.168.1.43 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 2979.0 failed 1 times, most recent failure: Lost task 9.0 in stage 2979.0 (TID 105026) (192.168.1.43 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Double.valueOf(Double.java:773)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:77)\n",
      "\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:107)\n",
      "\tat org.apache.spark.ml.tree.LearningNode.predictImpl(Node.scala:348)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:562)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa730.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.BitmapIndexedMapNode.foreach(HashMap.scala:1115)\n",
      "\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1122)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418fa360.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:657)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda/0x00007f33418ede30.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007f3340c71440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007f3340c608c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntrenando modelo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m m = \u001b[43mbuild_and_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m pred, mtx = evaluate_model(m, test, model_name=name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mbuild_and_fit\u001b[39m\u001b[34m(model_name, train_df)\u001b[39m\n\u001b[32m     49\u001b[39m tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid,\n\u001b[32m     50\u001b[39m                            evaluator=evaluator, trainRatio=\u001b[32m0.8\u001b[39m, parallelism=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m tvs_model = \u001b[43mtvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GBT sin weights\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tvs_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:1497\u001b[39m, in \u001b[36mTrainValidationSplit._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   1496\u001b[39m metrics = [\u001b[38;5;28;01mNone\u001b[39;00m] * numModels\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/multiprocessing/pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:1497\u001b[39m, in \u001b[36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m   1496\u001b[39m metrics = [\u001b[38;5;28;01mNone\u001b[39;00m] * numModels\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool.imap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[32m   1498\u001b[39m     metrics[j] = metric\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/util.py:435\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     session.addTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:115\u001b[39m, in \u001b[36m_parallelFitTasks.<locals>.singleTask\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingleTask\u001b[39m() -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     index, model = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:96\u001b[39m, in \u001b[36m_FitMultipleIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.counter += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:154\u001b[39m, in \u001b[36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) -> M:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:201\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/pipeline.py:138\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     transformers.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2205\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2202\u001b[39m         traceback.print_exc()\n\u001b[32m   2203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2207\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2208\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/ipykernel/zmqshell.py:587\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    581\u001b[39m sys.stdout.flush()\n\u001b[32m    582\u001b[39m sys.stderr.flush()\n\u001b[32m    584\u001b[39m exc_content = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    588\u001b[39m }\n\u001b[32m    590\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3474\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3470\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "metrics = {}\n",
    "predictions = {}\n",
    "\n",
    "for name in [\"lr\",\"rf\",\"gbt\"]:\n",
    "    print(f\"Entrenando modelo: {name}\")\n",
    "    m = build_and_fit(name, train)\n",
    "    pred, mtx = evaluate_model(m, test, model_name=name)\n",
    "    models[name] = m\n",
    "    metrics[name] = mtx\n",
    "    predictions[name] = pred.select(\"cliente_id\",\"ym\",\"label\",\"p_digital\")\n",
    "\n",
    "# Selección por AUC PR\n",
    "best_name = sorted(metrics.items(), key=lambda x: x[1][\"auc_pr\"], reverse=True)[0][0]\n",
    "best_model = models[best_name]\n",
    "best_pred  = predictions[best_name]\n",
    "\n",
    "print(\"\\n>>> Mejor modelo por AUC PR:\", best_name, metrics[best_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dec5ac",
   "metadata": {},
   "source": [
    "## 9) Tuning de umbral (max F1)\n",
    "\n",
    "Buscamos el umbral óptimo para el **mejor modelo**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_with_threshold_from_pred(pred_df, thr: float):\n",
    "    p = pred_df.withColumn(\"pred_thr\", (F.col(\"p_digital\") >= F.lit(thr)).cast(\"int\"))\n",
    "    agg = p.agg(\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"fn\")\n",
    "    ).first()\n",
    "    tp, tn, fp, fn = [float(agg[x] or 0.0) for x in (\"tp\",\"tn\",\"fp\",\"fn\")]\n",
    "    total = tp + tn + fp + fn or 1.0\n",
    "    accuracy  = (tp + tn) / total\n",
    "    precision = tp / (tp + fp or 1.0)\n",
    "    recall    = tp / (tp + fn or 1.0)\n",
    "    f1        = (2 * precision * recall) / (precision + recall or 1e-9)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "grid = [x/100 for x in range(30, 81, 5)]\n",
    "rows = []\n",
    "for t in grid:\n",
    "    acc, pre, rec, f1 = metrics_with_threshold_from_pred(best_pred, t)\n",
    "    rows.append((t, acc, pre, rec, f1))\n",
    "\n",
    "thr_df = spark.createDataFrame(rows, [\"threshold\",\"accuracy\",\"precision\",\"recall\",\"f1\"]).orderBy(F.desc(\"f1\"))\n",
    "thr_df.show(20, False)\n",
    "\n",
    "best_thr = thr_df.first()[\"threshold\"]\n",
    "acc, pre, rec, f1 = metrics_with_threshold_from_pred(best_pred, best_thr)\n",
    "print(f\"Mejor threshold: {best_thr:.2f} | Acc: {acc:.4f}  Prec: {pre:.4f}  Rec: {rec:.4f}  F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd06f7",
   "metadata": {},
   "source": [
    "## 10) Métricas por segmento (país / región)\n",
    "\n",
    "Evalúa **dónde** funciona mejor el modelo (AUC por grupo). Se omiten segmentos con una sola clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = (best_pred.join(test.select(\"cliente_id\",\"ym\",\"pais_cd\",\"region_comercial_txt\"),\n",
    "                           on=[\"cliente_id\",\"ym\"], how=\"left\"))\n",
    "\n",
    "def auc_by_group(df_in, group_col: str):\n",
    "    df_in = df_in.filter(F.col(group_col).isNotNull())\n",
    "    groups = [r[0] for r in df_in.select(group_col).distinct().collect()]\n",
    "    e_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"p_digital\", metricName=\"areaUnderROC\")\n",
    "    out = []\n",
    "    for g in groups:\n",
    "        subset = df_in.filter(F.col(group_col)==g)\n",
    "        if subset.select(\"label\").distinct().count() < 2:\n",
    "            continue\n",
    "        # Como 'p_digital' es probabilidad, usamos el evaluador con probabilidades: necesita una columna de rawPrediction.\n",
    "        # Truco: crear una columna vector con forma [1-p, p] sólo para el evaluador.\n",
    "        sub2 = subset.withColumn(\"rawPrediction\", F.array(1.0 - F.col(\"p_digital\"), F.col(\"p_digital\")))\n",
    "        auc_g = e_auc.evaluate(sub2)\n",
    "        out.append((g, float(auc_g)))\n",
    "    return spark.createDataFrame(out, [group_col, \"auc_roc\"]).orderBy(F.desc(\"auc_roc\"))\n",
    "\n",
    "print(\"AUC ROC por país:\")\n",
    "auc_by_group(joined, \"pais_cd\").show(truncate=False)\n",
    "\n",
    "print(\"AUC ROC por región:\")\n",
    "auc_by_group(joined, \"region_comercial_txt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291c625",
   "metadata": {},
   "source": [
    "## 11) Calibración de probabilidad (curva de confiabilidad)\n",
    "\n",
    "- Particionamos por **deciles** de probabilidad (`p_digital`).\n",
    "\n",
    "- Mostramos **promedio predicho vs. observado** por bin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84aa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntile(10) sobre probabilidad\n",
    "w_prob = Window.orderBy(F.col(\"p_digital\").asc())\n",
    "calib = (best_pred\n",
    "         .withColumn(\"decile\", F.ntile(10).over(w_prob))\n",
    "         .groupBy(\"decile\")\n",
    "         .agg(F.avg(\"p_digital\").alias(\"p_pred_mean\"),\n",
    "              F.avg(F.col(\"label\").cast(\"double\")).alias(\"p_obs_mean\"),\n",
    "              F.count(\"*\").alias(\"n\"))\n",
    "         .orderBy(\"decile\"))\n",
    "\n",
    "calib.show(10, False)\n",
    "\n",
    "# Plot opcional (si tienes matplotlib)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    pdf = calib.toPandas()\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(pdf[\"p_pred_mean\"], pdf[\"p_obs_mean\"], marker=\"o\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.title(\"Curva de calibración\")\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Observado\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Plot no disponible (matplotlib no instalado):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd90eb",
   "metadata": {},
   "source": [
    "## 12) Backtesting (opcional)\n",
    "\n",
    "Repite entrenamiento/evaluación en múltiples cortes temporales (`BACKTEST_SPLITS`) para verificar **estabilidad**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(ym_cut):\n",
    "    train_bt = ds.filter(F.col(\"ym\") < F.lit(ym_cut))\n",
    "    test_bt  = ds.filter(F.col(\"ym\") >= F.lit(ym_cut))\n",
    "    m = build_and_fit(best_name, train_bt)\n",
    "    pred_bt, mtx_bt = evaluate_model(m, test_bt, model_name=f\"{best_name}@{ym_cut}\")\n",
    "    return ym_cut, mtx_bt\n",
    "\n",
    "bt_rows = []\n",
    "for cut in BACKTEST_SPLITS:\n",
    "    try:\n",
    "        ym_cut, mtx = run_split(cut)\n",
    "        bt_rows.append((ym_cut, float(mtx[\"auc_pr\"]), float(mtx[\"auc_roc\"]), float(mtx[\"f1\"])))\n",
    "    except Exception as e:\n",
    "        print(f\"Backtest falló en corte {cut}:\", e)\n",
    "\n",
    "if bt_rows:\n",
    "    bt_df = spark.createDataFrame(bt_rows, [\"ym_cut\",\"auc_pr\",\"auc_roc\",\"f1\"]).orderBy(\"ym_cut\")\n",
    "    bt_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891a5d0",
   "metadata": {},
   "source": [
    "## 13) Exportar artefactos y métricas\n",
    "\n",
    "- Guarda el **mejor modelo** (`models/`).\n",
    "\n",
    "- Guarda **métricas** y **predicciones** (`results/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016381a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo\n",
    "models_dir = f\"models/{best_name}_next_digital_v3\"\n",
    "best_model.bestModel.write().overwrite().save(models_dir)\n",
    "\n",
    "# Guardar métricas globales\n",
    "from datetime import datetime\n",
    "metrics_rows = [\n",
    "    (\"model\", best_name),\n",
    "    (\"auc_pr\", float(metrics[best_name][\"auc_pr\"])),\n",
    "    (\"auc_roc\", float(metrics[best_name][\"auc_roc\"])),\n",
    "    (\"accuracy\", float(metrics[best_name][\"accuracy\"])),\n",
    "    (\"precision\", float(metrics[best_name][\"precision\"])),\n",
    "    (\"recall\", float(metrics[best_name][\"recall\"])),\n",
    "    (\"f1\", float(metrics[best_name][\"f1\"])),\n",
    "    (\"best_threshold\", float(best_thr)),\n",
    "    (\"generated_at\", datetime.utcnow().isoformat())\n",
    "]\n",
    "spark.createDataFrame([(k, v) for k, v in metrics_rows], [\"metric\",\"value\"]).coalesce(1)      .write.mode(\"overwrite\").json(\"results/metrics_v3\")\n",
    "\n",
    "# Guardar predicciones con probabilidad\n",
    "best_pred.write.mode(\"overwrite\").parquet(\"results/pred_test_v3\")\n",
    "\n",
    "print(\"Modelo guardado en:\", models_dir)\n",
    "print(\"Métricas y predicciones guardadas en 'results/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5120e10",
   "metadata": {},
   "source": [
    "## 14) Cierre de sesión Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()\n",
    "#print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844d2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
