{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd95f4f",
   "metadata": {},
   "source": [
    "# Modelado — Próximo pedido **DIGITAL** (v3 GPU, 100% PySpark)\n",
    "\n",
    "Este notebook intenta aprovechar tu **GPU NVIDIA RTX 3050** mediante **RAPIDS Accelerator for Apache Spark** si el plugin está disponible. Si no, hace **fallback a CPU** sin romperse.\n",
    "\n",
    "### Cómo ejecutarlo con GPU (Debian + PRIME)\n",
    "Lanza el entorno que use el kernel de Python con la dGPU:\n",
    "\n",
    "```bash\n",
    "prime-run jupyter lab\n",
    "# o\n",
    "prime-run code\n",
    "```\n",
    "\n",
    "> Si usas `spark-submit`, también: `prime-run spark-submit ...`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabf0a2",
   "metadata": {},
   "source": [
    "## 1) Sesión Spark (GPU primero, con fallback seguro a CPU)\n",
    "\n",
    "- Intentamos activar **RAPIDS** con:\n",
    "  - `spark.plugins = com.nvidia.spark.SQLPlugin`\n",
    "  - `spark.rapids.sql.enabled = true`\n",
    "  - Asignar GPU en local (`spark.task.resource.gpu.amount=1`)\n",
    "- Si no está el plugin, re-creamos Spark **en CPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42346dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "def create_spark_session(gpu_first: bool = True):\n",
    "    base = (SparkSession.builder\n",
    "            .appName(\"modelado-proximo-pedido-digital-v3-gpu\")\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "            .config(\"spark.driver.memory\", \"6g\")\n",
    "            .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\")\n",
    "           )\n",
    "    if gpu_first:\n",
    "        try:\n",
    "            gpu_builder = (base\n",
    "                .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "                .config(\"spark.rapids.sql.enabled\", \"true\")\n",
    "                .config(\"spark.rapids.sql.concurrentGpuTasks\", \"1\")\n",
    "                .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "                .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "                .config(\"spark.locality.wait\", \"0s\")\n",
    "            )\n",
    "            sp = gpu_builder.getOrCreate()\n",
    "            try:\n",
    "                sp._jvm.java.lang.Class.forName(\"com.nvidia.spark.SQLPlugin\")\n",
    "                mode = \"GPU (RAPIDS activo)\"\n",
    "            except Exception:\n",
    "                mode = \"GPU solicitado, pero plugin no detectado — ejecutando en CPU\"\n",
    "            print(\"Spark iniciado en modo:\", mode)\n",
    "            return sp\n",
    "        except Exception as e:\n",
    "            print(\"Fallo al iniciar con GPU/RAPIDS, usando CPU. Detalle:\", str(e))\n",
    "    sp = base.getOrCreate()\n",
    "    print(\"Spark iniciado en modo: CPU\")\n",
    "    return sp\n",
    "\n",
    "spark = create_spark_session(gpu_first=True)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"spark.plugins =\", spark.conf.get(\"spark.plugins\", None))\n",
    "print(\"spark.rapids.sql.enabled =\", spark.conf.get(\"spark.rapids.sql.enabled\", None))\n",
    "\n",
    "DATA_DIR = \"dataset/dataset\"      # ajusta si tu ruta cambia\n",
    "TEST_START_YM = \"2024-01\"         # Train < 2024-01 ; Test >= 2024-01\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3420989",
   "metadata": {},
   "source": [
    "## 2) Carga y preparación mínima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(DATA_DIR)\n",
    "\n",
    "expected = [\n",
    "    \"cliente_id\",\"pais_cd\",\"region_comercial_txt\",\"agencia_id\",\"ruta_id\",\n",
    "    \"tipo_cliente_cd\",\"madurez_digital_cd\",\"estrellas_txt\",\"frecuencia_visitas_cd\",\n",
    "    \"fecha_pedido_dt\",\"canal_pedido_cd\",\"facturacion_usd_val\",\n",
    "    \"materiales_distintos_val\",\"cajas_fisicas\"\n",
    "]\n",
    "present = [c for c in expected if c in df.columns]\n",
    "print(\"Columnas presentes:\", present)\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
    "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\")==\"DIGITAL\", 1).otherwise(0))\n",
    ")\n",
    "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da872c6c",
   "metadata": {},
   "source": [
    "## 3) Etiqueta por cliente-mes (determinista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ba234",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df.columns\n",
    "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc(),\n",
    "                                                          F.hash(*[F.col(c) for c in all_cols]).asc())\n",
    "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc(),\n",
    "                                                                             F.hash(*[F.col(c) for c in all_cols]).desc())\n",
    "\n",
    "orders = (df\n",
    "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
    "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
    "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
    "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
    "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
    ")\n",
    "\n",
    "last_in_month = (orders\n",
    "    .filter(F.col(\"rn_month_desc\")==1)\n",
    "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
    "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
    "            F.col(\"next_is_digital\").alias(\"label\"))\n",
    ")\n",
    "last_in_month.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbdfc9",
   "metadata": {},
   "source": [
    "## 4) Feature engineering (rolling 3m, crecimiento, ciclo de vida y priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_agg = (df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_orders\"),\n",
    "        F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
    "        F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
    "        F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
    "        F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
    "        F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
    "        F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
    "        F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
    "        F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
    "        F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
    "        F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
    "        F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
    "first_month = (monthly_agg\n",
    "               .withColumn(\"first_month\", F.first(\"month_first\", ignorenulls=True).over(w_client_month))\n",
    "               .select(\"cliente_id\",\"first_month\").distinct())\n",
    "\n",
    "monthly_agg = monthly_agg.join(first_month, on=\"cliente_id\", how=\"left\")                          .withColumn(\"months_since_first\", F.floor(F.months_between(\"month_first\", \"first_month\")))\n",
    "\n",
    "region_month = (df.groupBy(\"region_comercial_txt\",\"month_first\")\n",
    "                  .agg(F.avg(\"is_digital\").alias(\"region_digital_ratio\"))\n",
    "                  .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\")))\n",
    "w_region = Window.partitionBy(\"region_comercial_txt\").orderBy(F.col(\"month_first\").asc())\n",
    "region_month = region_month.withColumn(\"region_digital_ratio_lag1\", F.lag(\"region_digital_ratio\", 1).over(w_region))                            .select(\"region_comercial_txt\",\"ym\",\"region_digital_ratio_lag1\")\n",
    "\n",
    "tipo_month = (df.groupBy(\"tipo_cliente_cd\",\"month_first\")\n",
    "                .agg(F.avg(\"is_digital\").alias(\"tipo_digital_ratio\"))\n",
    "                .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\")))\n",
    "w_tipo = Window.partitionBy(\"tipo_cliente_cd\").orderBy(F.col(\"month_first\").asc())\n",
    "tipo_month = tipo_month.withColumn(\"tipo_digital_ratio_lag1\", F.lag(\"tipo_digital_ratio\", 1).over(w_tipo))                        .select(\"tipo_cliente_cd\",\"ym\",\"tipo_digital_ratio_lag1\")\n",
    "\n",
    "w_roll3 = w_client_month.rowsBetween(-3, -1)\n",
    "ds = (monthly_agg\n",
    "    .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
    "    .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
    "    .withColumn(\"n_orders_3m\", F.sum(\"n_orders\").over(w_roll3))\n",
    "    .withColumn(\"digital_ratio_3m\", F.avg(\"digital_ratio\").over(w_roll3))\n",
    "    .withColumn(\"sum_fact_3m\", F.sum(\"sum_fact\").over(w_roll3))\n",
    "    .withColumn(\"growth_digital_ratio\", F.col(\"digital_ratio\") - F.col(\"lag1_digital_ratio\"))\n",
    "    .join(region_month, on=[\"region_comercial_txt\",\"ym\"], how=\"left\")\n",
    "    .join(tipo_month, on=[\"tipo_cliente_cd\",\"ym\"], how=\"left\")\n",
    "    .filter(F.col(\"label\").isNotNull())\n",
    ")\n",
    "ds.select(\"cliente_id\",\"ym\",\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\n",
    "          \"n_orders_3m\",\"digital_ratio_3m\",\"growth_digital_ratio\",\n",
    "          \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\",\n",
    "          \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eac8f",
   "metadata": {},
   "source": [
    "## 5) Split temporal + balance de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d800530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.filter(F.col(\"ym\") < F.lit(TEST_START_YM))\n",
    "test  = ds.filter(F.col(\"ym\") >= F.lit(TEST_START_YM))\n",
    "\n",
    "print(\"Train rows:\", train.count(), \" | Test rows:\", test.count())\n",
    "print(\"\\nDistribución de la etiqueta:\")\n",
    "for name, d in [(\"train\", train), (\"test\", test)]:\n",
    "    print(f\"--- {name} ---\"); d.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ac6f6",
   "metadata": {},
   "source": [
    "## 6) Pipeline (LR, RF, GBT) y comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\n",
    "            \"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\",\n",
    "            \"n_orders_3m\",\"digital_ratio_3m\",\"sum_fact_3m\",\"growth_digital_ratio\",\n",
    "            \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\"]\n",
    "cat_cols = [\"tipo_cliente_cd\",\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"region_comercial_txt\"]\n",
    "\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=[c + \"_imp\" for c in num_cols])\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[c + \"_idx\"], outputCols=[c + \"_oh\"]) for c in cat_cols]\n",
    "feature_cols = [c + \"_imp\" for c in num_cols] + [c + \"_oh\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "def build_and_fit(model_name: str, train_df):\n",
    "    pos = train_df.filter(F.col(\"label\")==1).count()\n",
    "    neg = train_df.filter(F.col(\"label\")==0).count()\n",
    "    balancing_ratio = neg / float(max(pos, 1)) if pos else 1.0\n",
    "    train_w = train_df.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(balancing_ratio)).otherwise(F.lit(1.0)))\n",
    "\n",
    "    if model_name == \"lr\":\n",
    "        clf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                 maxIter=80, regParam=0.01, elasticNetParam=0.0)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.regParam, [0.0, 0.01, 0.1])\n",
    "                     .addGrid(clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                     .build())\n",
    "    elif model_name == \"rf\":\n",
    "        clf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                     numTrees=200, maxDepth=10, featureSubsetStrategy=\"sqrt\",\n",
    "                                     subsamplingRate=0.8, seed=42)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.numTrees, [150, 200, 300])\n",
    "                     .addGrid(clf.maxDepth, [8, 10, 12])\n",
    "                     .build())\n",
    "    elif model_name == \"gbt\":\n",
    "        clf = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=100, maxDepth=6, stepSize=0.1, seed=42)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.maxIter, [60, 100])\n",
    "                     .addGrid(clf.maxDepth, [5, 6, 8])\n",
    "                     .build())\n",
    "    else:\n",
    "        raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler, clf])\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                               evaluator=evaluator, trainRatio=0.8, parallelism=2)\n",
    "    tvs_model = tvs.fit(train_w if model_name in (\"lr\",\"rf\") else train_df)  # GBT sin weights (depende versión)\n",
    "    return tvs_model\n",
    "\n",
    "def evaluate_model(model, test_df, model_name=\"model\"):\n",
    "    pred = model.transform(test_df).withColumn(\"p_digital\", vector_to_array(\"probability\")[1]).cache()\n",
    "    e_auc  = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    e_aupr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    auc = e_auc.evaluate(pred); aupr = e_aupr.evaluate(pred)\n",
    "    cm = pred.groupBy(\"label\",\"prediction\").count().toPandas()\n",
    "    tp = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    tn = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "    fp = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    fn = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "    accuracy  = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall    = tp / max(tp + fn, 1)\n",
    "    f1        = (2 * precision * recall) / max(precision + recall, 1e-9)\n",
    "    print(f\"[{model_name}]  AUC ROC: {auc:.4f} | AUC PR: {aupr:.4f} | Acc: {accuracy:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    return pred, {\"auc_roc\": auc, \"auc_pr\": aupr, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "models, metrics, predictions = {}, {}, {}\n",
    "for name in [\"lr\",\"rf\",\"gbt\"]:\n",
    "    print(f\"Entrenando modelo: {name}\")\n",
    "    m = build_and_fit(name, train)\n",
    "    pred, mtx = evaluate_model(m, test, model_name=name)\n",
    "    models[name] = m; metrics[name] = mtx; predictions[name] = pred.select(\"cliente_id\",\"ym\",\"label\",\"p_digital\")\n",
    "\n",
    "best_name = sorted(metrics.items(), key=lambda x: x[1][\"auc_pr\"], reverse=True)[0][0]\n",
    "best_model = models[best_name]\n",
    "best_pred  = predictions[best_name]\n",
    "print(\"\\n>>> Mejor modelo por AUC PR:\", best_name, metrics[best_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063baf02",
   "metadata": {},
   "source": [
    "## 7) Tuning de umbral (max F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_with_threshold_from_pred(pred_df, thr: float):\n",
    "    p = pred_df.withColumn(\"pred_thr\", (F.col(\"p_digital\") >= F.lit(thr)).cast(\"int\"))\n",
    "    agg = p.agg(\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"fn\")\n",
    "    ).first()\n",
    "    tp, tn, fp, fn = [float(agg[x] or 0.0) for x in (\"tp\",\"tn\",\"fp\",\"fn\")]\n",
    "    total = tp + tn + fp + fn or 1.0\n",
    "    accuracy  = (tp + tn) / total\n",
    "    precision = tp / (tp + fp or 1.0)\n",
    "    recall    = tp / (tp + fn or 1.0)\n",
    "    f1        = (2 * precision * recall) / (precision + recall or 1e-9)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "grid = [x/100 for x in range(30, 81, 5)]\n",
    "rows = [(t,)+metrics_with_threshold_from_pred(best_pred, t) for t in grid]\n",
    "thr_df = spark.createDataFrame(rows, [\"threshold\",\"accuracy\",\"precision\",\"recall\",\"f1\"]).orderBy(F.desc(\"f1\"))\n",
    "thr_df.show(20, False)\n",
    "\n",
    "best_thr = thr_df.first()[\"threshold\"]\n",
    "acc, pre, rec, f1 = metrics_with_threshold_from_pred(best_pred, best_thr)\n",
    "print(f\"Mejor threshold: {best_thr:.2f} | Acc: {acc:.4f}  Prec: {pre:.4f}  Rec: {rec:.4f}  F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2da59c",
   "metadata": {},
   "source": [
    "## 8) Exportar artefactos y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39946864",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{best_name}_next_digital_v3_gpu\"\n",
    "best_model.bestModel.write().overwrite().save(models_dir)\n",
    "\n",
    "from datetime import datetime\n",
    "metrics_rows = [\n",
    "    (\"model\", best_name),\n",
    "    (\"auc_pr\", float(metrics[best_name][\"auc_pr\"])),\n",
    "    (\"auc_roc\", float(metrics[best_name][\"auc_roc\"])),\n",
    "    (\"accuracy\", float(metrics[best_name][\"accuracy\"])),\n",
    "    (\"precision\", float(metrics[best_name][\"precision\"])),\n",
    "    (\"recall\", float(metrics[best_name][\"recall\"])),\n",
    "    (\"f1\", float(metrics[best_name][\"f1\"])),\n",
    "    (\"best_threshold\", float(best_thr)),\n",
    "    (\"generated_at\", datetime.utcnow().isoformat())\n",
    "]\n",
    "spark.createDataFrame([(k, v) for k, v in metrics_rows], [\"metric\",\"value\"]).coalesce(1)      .write.mode(\"overwrite\").json(\"results/metrics_v3_gpu\")\n",
    "\n",
    "best_pred.write.mode(\"overwrite\").parquet(\"results/pred_test_v3_gpu\")\n",
    "\n",
    "print(\"Modelo guardado en:\", models_dir)\n",
    "print(\"Métricas y predicciones guardadas en 'results/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067672b",
   "metadata": {},
   "source": [
    "## 9) Cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
