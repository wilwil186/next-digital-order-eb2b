{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd95f4f",
   "metadata": {},
   "source": [
    "# Modelado — Próximo pedido **DIGITAL** (v3 GPU, 100% PySpark)\n",
    "\n",
    "Este notebook intenta aprovechar tu **GPU NVIDIA RTX 3050** mediante **RAPIDS Accelerator for Apache Spark** si el plugin está disponible. Si no, hace **fallback a CPU** sin romperse.\n",
    "\n",
    "### Cómo ejecutarlo con GPU (Debian + PRIME)\n",
    "Lanza el entorno que use el kernel de Python con la dGPU:\n",
    "\n",
    "```bash\n",
    "prime-run jupyter lab\n",
    "# o\n",
    "prime-run code\n",
    "```\n",
    "\n",
    "> Si usas `spark-submit`, también: `prime-run spark-submit ...`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabf0a2",
   "metadata": {},
   "source": [
    "## 1) Sesión Spark (GPU primero, con fallback seguro a CPU)\n",
    "\n",
    "- Intentamos activar **RAPIDS** con:\n",
    "  - `spark.plugins = com.nvidia.spark.SQLPlugin`\n",
    "  - `spark.rapids.sql.enabled = true`\n",
    "  - Asignar GPU en local (`spark.task.resource.gpu.amount=1`)\n",
    "- Si no está el plugin, re-creamos Spark **en CPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dad3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/16 20:33:35 WARN Utils: Your hostname, debian, resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface wlo1)\n",
      "25/09/16 20:33:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/wilson/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/wilson/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/wilson/.ivy2.5.2/jars\n",
      "com.nvidia#rapids-4-spark_2.13 added as a dependency\n",
      "ai.rapids#cudf added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4279d5f2-e801-4109-922e-14abdfad0cf8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.nvidia#rapids-4-spark_2.13;25.08.0 in central\n",
      "\tfound ai.rapids#cudf;25.08.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "downloading https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.13/25.08.0/rapids-4-spark_2.13-25.08.0.jar ...\n",
      "\t[SUCCESSFUL ] com.nvidia#rapids-4-spark_2.13;25.08.0!rapids-4-spark_2.13.jar (186567ms)\n",
      "downloading https://repo1.maven.org/maven2/ai/rapids/cudf/25.08.0/cudf-25.08.0.jar ...\n",
      "\t[SUCCESSFUL ] ai.rapids#cudf;25.08.0!cudf.jar (95708ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (316ms)\n",
      ":: resolution report :: resolve 23287ms :: artifacts dl 282597ms\n",
      "\t:: modules in use:\n",
      "\tai.rapids#cudf;25.08.0 from central in [default]\n",
      "\tcom.nvidia#rapids-4-spark_2.13;25.08.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4279d5f2-e801-4109-922e-14abdfad0cf8\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (1239075kB/602ms)\n",
      "25/09/16 20:38:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 20:38:43 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalArgumentException: This RAPIDS Plugin build does not support Spark build 4.0.1. Supported Spark versions: 3.3.0 {buildver=330}, 3.3.1 {buildver=331}, 3.3.2 {buildver=332}, 3.3.3 {buildver=333}, 3.3.4 {buildver=334}, 3.4.0 {buildver=340}, 3.4.1 {buildver=341}, 3.4.2 {buildver=342}, 3.4.3 {buildver=343}, 3.4.4 {buildver=344}, 3.5.0 {buildver=350}, 3.5.1 {buildver=351}, 3.5.2 {buildver=352}, 3.5.3 {buildver=353}, 3.5.4 {buildver=354}, 3.5.5 {buildver=355}, 3.5.6 {buildver=356}, 4.0.0 {buildver=400}. Consult the Release documentation at https://nvidia.github.io/spark-rapids/docs/download.html\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.$anonfun$detectShimProvider$13(ShimLoader.scala:287)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.detectShimProvider(ShimLoader.scala:277)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.findShimProvider(ShimLoader.scala:298)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.initShimProviderIfNeeded(ShimLoader.scala:91)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.getShimClassLoader(ShimLoader.scala:183)\n",
      "\tat com.nvidia.spark.rapids.ShimReflectionUtils$.loadClass(ShimReflectionUtils.scala:28)\n",
      "\tat com.nvidia.spark.rapids.ShimReflectionUtils$.newInstanceOf(ShimReflectionUtils.scala:34)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.newDriverPlugin(ShimLoader.scala:334)\n",
      "\tat com.nvidia.spark.SQLPlugin.driverPlugin(SQLPlugin.scala:28)\n",
      "\tat org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:47)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.flatMap(StrictOptimizedIterableOps.scala:118)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.flatMap$(StrictOptimizedIterableOps.scala:105)\n",
      "\tat scala.collection.immutable.ArraySeq.flatMap(ArraySeq.scala:35)\n",
      "\tat org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:46)\n",
      "\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:213)\n",
      "\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:196)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:588)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/09/16 20:38:43 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n",
      "25/09/16 20:38:43 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No se pudo activar RAPIDS (GPU). Motivo: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": java.lang.IllegalArgumentException: This RAPIDS Plugin build does not support Spark build 4.0.1. Supported Spark versions: 3.3.0 {buildver=330}, 3.3.1 {buildver=331}, 3.3.2 {buildver=332}, 3.3.3 {buildver=333}, 3.3.4 {buildver=334}, 3.4.0 {buildver=340}, 3.4.1 {buildver=341}, 3.4.2 {buildver=342}, 3.4.3 {buildver=343}, 3.4.4 {buildver=344}, 3.5.0 {buildver=350}, 3.5.1 {buildver=351}, 3.5.2 {buildver=352}, 3.5.3 {buildver=353}, 3.5.4 {buildver=354}, 3.5.5 {buildver=355}, 3.5.6 {buildver=356}, 4.0.0 {buildver=400}. Consult the Release documentation at https://nvidia.github.io/spark-rapids/docs/download.html\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.$anonfun$detectShimProvider$13(ShimLoader.scala:287)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.detectShimProvider(ShimLoader.scala:277)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.findShimProvider(ShimLoader.scala:298)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.initShimProviderIfNeeded(ShimLoader.scala:91)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.getShimClassLoader(ShimLoader.scala:183)\n",
      "\tat com.nvidia.spark.rapids.ShimReflectionUtils$.loadClass(ShimReflectionUtils.scala:28)\n",
      "\tat com.nvidia.spark.rapids.ShimReflectionUtils$.newInstanceOf(ShimReflectionUtils.scala:34)\n",
      "\tat com.nvidia.spark.rapids.ShimLoader$.newDriverPlugin(ShimLoader.scala:334)\n",
      "\tat com.nvidia.spark.SQLPlugin.driverPlugin(SQLPlugin.scala:28)\n",
      "\tat org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:47)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.flatMap(StrictOptimizedIterableOps.scala:118)\n",
      "\tat scala.collection.StrictOptimizedIterableOps.flatMap$(StrictOptimizedIterableOps.scala:105)\n",
      "\tat scala.collection.immutable.ArraySeq.flatMap(ArraySeq.scala:35)\n",
      "\tat org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:46)\n",
      "\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:213)\n",
      "\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:196)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:588)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 20:38:44 WARN ResourceUtils: The configuration of cores (exec = 12 task = 1, runnable tasks = 12) will result in wasted resources due to resource gpu limiting the number of runnable tasks per executor to: 1. Please adjust your configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Spark en CPU (sin RAPIDS).\n",
      "spark.plugins            = \n",
      "spark.rapids.sql.enabled = true\n",
      "Spark version (runtime)  = 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# === Sesión Spark robusta para VS Code (GPU si hay RAPIDS compatible; si no, CPU) ===\n",
    "import os, re\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# ---- Parámetros generales (ajusta según tu entorno) ----\n",
    "USE_MAVEN_AUTODOWNLOAD = True   # True: baja jars desde Maven; False: asume jars ya en ${SPARK_HOME}/jars\n",
    "USE_GPU = True                  # Cambia a False para forzar CPU\n",
    "DRIVER_MEMORY = \"6g\"\n",
    "\n",
    "# Tabla de compatibilidad rápida (ajusta si usas otras versiones)\n",
    "# Spark 4.x => Scala 2.13, RAPIDS >= 25.08.0 (ejemplo)\n",
    "# Spark 3.x => Scala 2.12, RAPIDS 24.12.0 (línea estable)\n",
    "RAPIDS_MATRIX = {\n",
    "    4: {\"RAPIDS_VER\": \"25.08.0\", \"SCALA_BIN\": \"2.13\"},\n",
    "    3: {\"RAPIDS_VER\": \"24.12.0\", \"SCALA_BIN\": \"2.12\"},\n",
    "}\n",
    "DEFAULT_FALLBACK = {\"RAPIDS_VER\": \"24.12.0\", \"SCALA_BIN\": \"2.12\"}  # por si acaso\n",
    "\n",
    "def _spark_major_runtime() -> int:\n",
    "    \"\"\"\n",
    "    Detecta la versión mayor del *runtime* de Spark:\n",
    "    - Si ya hay sesión activa, usa spark.version (fiable).\n",
    "    - Si no, usa pyspark.__version__ como fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sess = SparkSession.getActiveSession()\n",
    "        if sess is not None:\n",
    "            return int(sess.version.split(\".\")[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return int(re.match(r\"^(\\d+)\\.\", pyspark.__version__).group(1))\n",
    "\n",
    "def _build_base(app_name=\"modelado-proximo-pedido-digital-vscode\"):\n",
    "    return (SparkSession.builder\n",
    "            .appName(app_name)\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "            .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "            .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\"))\n",
    "\n",
    "def create_spark_vscode():\n",
    "    major = _spark_major_runtime()\n",
    "    compat = RAPIDS_MATRIX.get(major, DEFAULT_FALLBACK)\n",
    "    RAPIDS_VER = compat[\"RAPIDS_VER\"]\n",
    "    SCALA_BIN  = compat[\"SCALA_BIN\"]\n",
    "\n",
    "    # Si fuerzas CPU, no cargues plugin ni jars\n",
    "    if not USE_GPU:\n",
    "        sp = (_build_base(app_name=\"modelado-proximo-pedido-digital-vscode-cpu\")\n",
    "              .config(\"spark.plugins\", \"\")\n",
    "              .getOrCreate())\n",
    "        print(f\"➡️  Spark {sp.version} en CPU (GPU desactivada por configuración).\")\n",
    "        return sp\n",
    "\n",
    "    # Intentar GPU con jars vía Maven (requiere internet)\n",
    "    b = (_build_base(app_name=\"modelado-proximo-pedido-digital-vscode-gpu\")\n",
    "         .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "         .config(\"spark.rapids.sql.enabled\", \"true\")\n",
    "         .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "         .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "         .config(\"spark.rapids.sql.concurrentGpuTasks\", \"1\")\n",
    "         .config(\"spark.locality.wait\", \"0s\"))\n",
    "\n",
    "    if USE_MAVEN_AUTODOWNLOAD:\n",
    "        b = b.config(\n",
    "            \"spark.jars.packages\",\n",
    "            f\"com.nvidia:rapids-4-spark_{SCALA_BIN}:{RAPIDS_VER},\"\n",
    "            f\"ai.rapids:cudf:{RAPIDS_VER}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        sp = b.getOrCreate()\n",
    "        # Verifica que el plugin esté realmente en el classpath\n",
    "        sp._jvm.java.lang.Class.forName(\"com.nvidia.spark.SQLPlugin\")\n",
    "        print(f\"✅ Spark {sp.version} + RAPIDS {RAPIDS_VER} activo (GPU, Scala {SCALA_BIN}).\")\n",
    "        # Para diagnosticar qué no cae en GPU:\n",
    "        sp.conf.set(\"spark.rapids.sql.explain\", \"NOT_ON_GPU\")\n",
    "        return sp\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ No se pudo activar RAPIDS (GPU). Motivo:\", str(e))\n",
    "\n",
    "    # Fallback CPU limpio\n",
    "    sp = (_build_base(app_name=\"modelado-proximo-pedido-digital-vscode-cpu\")\n",
    "          .config(\"spark.plugins\", \"\")\n",
    "          .getOrCreate())\n",
    "    print(\"➡️  Spark en CPU (sin RAPIDS).\")\n",
    "    return sp\n",
    "\n",
    "# ---- Crear sesión ----\n",
    "spark = create_spark_vscode()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ---- Info útil de arranque ----\n",
    "print(\"spark.plugins            =\", spark.conf.get(\"spark.plugins\", None))\n",
    "print(\"spark.rapids.sql.enabled =\", spark.conf.get(\"spark.rapids.sql.enabled\", None))\n",
    "print(\"Spark version (runtime)  =\", spark.version)\n",
    "\n",
    "# ---- Ajustes de tu flujo (ajusta rutas y cortes de train/test) ----\n",
    "DATA_DIR = \"dataset/dataset\"      # cambia si tu ruta es distinta\n",
    "TEST_START_YM = \"2024-01\"         # Train < 2024-01 ; Test >= 2024-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3420989",
   "metadata": {},
   "source": [
    "## 2) Carga y preparación mínima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e8e26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wilson/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/wilson/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m expected = [\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcliente_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mpais_cd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mregion_comercial_txt\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33magencia_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mruta_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtipo_cliente_cd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmTu salida mostró que ejecutaste !prime-run jupyter lab dentro de un notebook, lo cual solo imprime la ayuda. Lánzalo desde la terminal que inicia el servidor Jupyter:adurez_digital_cd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mestrellas_txt\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfrecuencia_visitas_cd\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfecha_pedido_dt\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcanal_pedido_cd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfacturacion_usd_val\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmateriales_distintos_val\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcajas_fisicas\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m ]\n\u001b[32m      9\u001b[39m present = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m expected \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/next-digital-order-eb2b/.venv/lib/python3.13/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(DATA_DIR)\n",
    "\n",
    "expected = [\n",
    "    \"cliente_id\",\"pais_cd\",\"region_comercial_txt\",\"agencia_id\",\"ruta_id\",\n",
    "    \"tipo_cliente_cd\",\"mTu salida mostró que ejecutaste !prime-run jupyter lab dentro de un notebook, lo cual solo imprime la ayuda. Lánzalo desde la terminal que inicia el servidor Jupyter:adurez_digital_cd\",\"estrellas_txt\",\"frecuencia_visitas_cd\",\n",
    "    \"fecha_pedido_dt\",\"canal_pedido_cd\",\"facturacion_usd_val\",\n",
    "    \"materiales_distintos_val\",\"cajas_fisicas\"\n",
    "]\n",
    "present = [c for c in expected if c in df.columns]\n",
    "print(\"Columnas presentes:\", present)\n",
    "\n",
    "df = (df\n",
    "    .withColumn(\"month_first\", F.trunc(\"fecha_pedido_dt\", \"month\"))\n",
    "    .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\"))\n",
    "    .withColumn(\"is_digital\", F.when(F.col(\"canal_pedido_cd\")==\"DIGITAL\", 1).otherwise(0))\n",
    ")\n",
    "df.select(\"cliente_id\",\"fecha_pedido_dt\",\"ym\",\"canal_pedido_cd\",\"is_digital\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da872c6c",
   "metadata": {},
   "source": [
    "## 3) Etiqueta por cliente-mes (determinista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ba234",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df.columns\n",
    "w_client_order = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"fecha_pedido_dt\").asc(),\n",
    "                                                          F.hash(*[F.col(c) for c in all_cols]).asc())\n",
    "w_client_month_desc = Window.partitionBy(\"cliente_id\",\"month_first\").orderBy(F.col(\"fecha_pedido_dt\").desc(),\n",
    "                                                                             F.hash(*[F.col(c) for c in all_cols]).desc())\n",
    "\n",
    "orders = (df\n",
    "    .withColumn(\"prev_dt\", F.lag(\"fecha_pedido_dt\").over(w_client_order))\n",
    "    .withColumn(\"next_canal\", F.lead(\"canal_pedido_cd\").over(w_client_order))\n",
    "    .withColumn(\"next_is_digital\", F.when(F.col(\"next_canal\")==\"DIGITAL\", 1).otherwise(0))\n",
    "    .withColumn(\"recency_days\", F.datediff(F.col(\"fecha_pedido_dt\"), F.col(\"prev_dt\")))\n",
    "    .withColumn(\"rn_month_desc\", F.row_number().over(w_client_month_desc))\n",
    ")\n",
    "\n",
    "last_in_month = (orders\n",
    "    .filter(F.col(\"rn_month_desc\")==1)\n",
    "    .select(\"cliente_id\",\"month_first\",\"ym\",\n",
    "            F.col(\"recency_days\").alias(\"recency_days_last\"),\n",
    "            F.col(\"next_is_digital\").alias(\"label\"))\n",
    ")\n",
    "last_in_month.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbdfc9",
   "metadata": {},
   "source": [
    "## 4) Feature engineering (rolling 3m, crecimiento, ciclo de vida y priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_agg = (df.groupBy(\"cliente_id\",\"month_first\",\"ym\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_orders\"),\n",
    "        F.avg(\"is_digital\").alias(\"digital_ratio\"),\n",
    "        F.sum(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"sum_fact\"),\n",
    "        F.avg(F.col(\"facturacion_usd_val\").cast(\"double\")).alias(\"avg_fact\"),\n",
    "        F.sum(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"sum_cajas\"),\n",
    "        F.avg(F.col(\"cajas_fisicas\").cast(\"double\")).alias(\"avg_cajas\"),\n",
    "        F.avg(F.col(\"materiales_distintos_val\").cast(\"double\")).alias(\"avg_mat_dist\"),\n",
    "        F.first(\"tipo_cliente_cd\", ignorenulls=True).alias(\"tipo_cliente_cd\"),\n",
    "        F.first(\"madurez_digital_cd\", ignorenulls=True).alias(\"madurez_digital_cd\"),\n",
    "        F.first(\"frecuencia_visitas_cd\", ignorenulls=True).alias(\"frecuencia_visitas_cd\"),\n",
    "        F.first(\"pais_cd\", ignorenulls=True).alias(\"pais_cd\"),\n",
    "        F.first(\"region_comercial_txt\", ignorenulls=True).alias(\"region_comercial_txt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_client_month = Window.partitionBy(\"cliente_id\").orderBy(F.col(\"month_first\").asc())\n",
    "first_month = (monthly_agg\n",
    "               .withColumn(\"first_month\", F.first(\"month_first\", ignorenulls=True).over(w_client_month))\n",
    "               .select(\"cliente_id\",\"first_month\").distinct())\n",
    "\n",
    "monthly_agg = monthly_agg.join(first_month, on=\"cliente_id\", how=\"left\")                          .withColumn(\"months_since_first\", F.floor(F.months_between(\"month_first\", \"first_month\")))\n",
    "\n",
    "region_month = (df.groupBy(\"region_comercial_txt\",\"month_first\")\n",
    "                  .agg(F.avg(\"is_digital\").alias(\"region_digital_ratio\"))\n",
    "                  .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\")))\n",
    "w_region = Window.partitionBy(\"region_comercial_txt\").orderBy(F.col(\"month_first\").asc())\n",
    "region_month = region_month.withColumn(\"region_digital_ratio_lag1\", F.lag(\"region_digital_ratio\", 1).over(w_region))                            .select(\"region_comercial_txt\",\"ym\",\"region_digital_ratio_lag1\")\n",
    "\n",
    "tipo_month = (df.groupBy(\"tipo_cliente_cd\",\"month_first\")\n",
    "                .agg(F.avg(\"is_digital\").alias(\"tipo_digital_ratio\"))\n",
    "                .withColumn(\"ym\", F.date_format(\"month_first\", \"yyyy-MM\")))\n",
    "w_tipo = Window.partitionBy(\"tipo_cliente_cd\").orderBy(F.col(\"month_first\").asc())\n",
    "tipo_month = tipo_month.withColumn(\"tipo_digital_ratio_lag1\", F.lag(\"tipo_digital_ratio\", 1).over(w_tipo))                        .select(\"tipo_cliente_cd\",\"ym\",\"tipo_digital_ratio_lag1\")\n",
    "\n",
    "w_roll3 = w_client_month.rowsBetween(-3, -1)\n",
    "ds = (monthly_agg\n",
    "    .join(last_in_month, on=[\"cliente_id\",\"month_first\",\"ym\"], how=\"left\")\n",
    "    .withColumn(\"lag1_digital_ratio\", F.lag(\"digital_ratio\", 1).over(w_client_month))\n",
    "    .withColumn(\"n_orders_3m\", F.sum(\"n_orders\").over(w_roll3))\n",
    "    .withColumn(\"digital_ratio_3m\", F.avg(\"digital_ratio\").over(w_roll3))\n",
    "    .withColumn(\"sum_fact_3m\", F.sum(\"sum_fact\").over(w_roll3))\n",
    "    .withColumn(\"growth_digital_ratio\", F.col(\"digital_ratio\") - F.col(\"lag1_digital_ratio\"))\n",
    "    .join(region_month, on=[\"region_comercial_txt\",\"ym\"], how=\"left\")\n",
    "    .join(tipo_month, on=[\"tipo_cliente_cd\",\"ym\"], how=\"left\")\n",
    "    .filter(F.col(\"label\").isNotNull())\n",
    ")\n",
    "ds.select(\"cliente_id\",\"ym\",\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\n",
    "          \"n_orders_3m\",\"digital_ratio_3m\",\"growth_digital_ratio\",\n",
    "          \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\",\n",
    "          \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eac8f",
   "metadata": {},
   "source": [
    "## 5) Split temporal + balance de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d800530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.filter(F.col(\"ym\") < F.lit(TEST_START_YM))\n",
    "test  = ds.filter(F.col(\"ym\") >= F.lit(TEST_START_YM))\n",
    "\n",
    "print(\"Train rows:\", train.count(), \" | Test rows:\", test.count())\n",
    "print(\"\\nDistribución de la etiqueta:\")\n",
    "for name, d in [(\"train\", train), (\"test\", test)]:\n",
    "    print(f\"--- {name} ---\"); d.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ac6f6",
   "metadata": {},
   "source": [
    "## 6) Pipeline (LR, RF, GBT) y comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"n_orders\",\"digital_ratio\",\"lag1_digital_ratio\",\"sum_fact\",\"avg_fact\",\n",
    "            \"sum_cajas\",\"avg_cajas\",\"avg_mat_dist\",\"recency_days_last\",\n",
    "            \"n_orders_3m\",\"digital_ratio_3m\",\"sum_fact_3m\",\"growth_digital_ratio\",\n",
    "            \"months_since_first\",\"region_digital_ratio_lag1\",\"tipo_digital_ratio_lag1\"]\n",
    "cat_cols = [\"tipo_cliente_cd\",\"madurez_digital_cd\",\"frecuencia_visitas_cd\",\"pais_cd\",\"region_comercial_txt\"]\n",
    "\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=[c + \"_imp\" for c in num_cols])\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[c + \"_idx\"], outputCols=[c + \"_oh\"]) for c in cat_cols]\n",
    "feature_cols = [c + \"_imp\" for c in num_cols] + [c + \"_oh\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "def build_and_fit(model_name: str, train_df):\n",
    "    pos = train_df.filter(F.col(\"label\")==1).count()\n",
    "    neg = train_df.filter(F.col(\"label\")==0).count()\n",
    "    balancing_ratio = neg / float(max(pos, 1)) if pos else 1.0\n",
    "    train_w = train_df.withColumn(\"weight\", F.when(F.col(\"label\")==1, F.lit(balancing_ratio)).otherwise(F.lit(1.0)))\n",
    "\n",
    "    if model_name == \"lr\":\n",
    "        clf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                 maxIter=80, regParam=0.01, elasticNetParam=0.0)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.regParam, [0.0, 0.01, 0.1])\n",
    "                     .addGrid(clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                     .build())\n",
    "    elif model_name == \"rf\":\n",
    "        clf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\",\n",
    "                                     numTrees=200, maxDepth=10, featureSubsetStrategy=\"sqrt\",\n",
    "                                     subsamplingRate=0.8, seed=42)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.numTrees, [150, 200, 300])\n",
    "                     .addGrid(clf.maxDepth, [8, 10, 12])\n",
    "                     .build())\n",
    "    elif model_name == \"gbt\":\n",
    "        clf = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=100, maxDepth=6, stepSize=0.1, seed=42)\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                     .addGrid(clf.maxIter, [60, 100])\n",
    "                     .addGrid(clf.maxDepth, [5, 6, 8])\n",
    "                     .build())\n",
    "    else:\n",
    "        raise ValueError(\"Modelo no soportado\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler, clf])\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                               evaluator=evaluator, trainRatio=0.8, parallelism=2)\n",
    "    tvs_model = tvs.fit(train_w if model_name in (\"lr\",\"rf\") else train_df)  # GBT sin weights (depende versión)\n",
    "    return tvs_model\n",
    "\n",
    "def evaluate_model(model, test_df, model_name=\"model\"):\n",
    "    pred = model.transform(test_df).withColumn(\"p_digital\", vector_to_array(\"probability\")[1]).cache()\n",
    "    e_auc  = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    e_aupr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    auc = e_auc.evaluate(pred); aupr = e_aupr.evaluate(pred)\n",
    "    cm = pred.groupBy(\"label\",\"prediction\").count().toPandas()\n",
    "    tp = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    tn = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "    fp = int(cm[(cm[\"label\"]==0) & (cm[\"prediction\"]==1)][\"count\"].sum())\n",
    "    fn = int(cm[(cm[\"label\"]==1) & (cm[\"prediction\"]==0)][\"count\"].sum())\n",
    "    accuracy  = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall    = tp / max(tp + fn, 1)\n",
    "    f1        = (2 * precision * recall) / max(precision + recall, 1e-9)\n",
    "    print(f\"[{model_name}]  AUC ROC: {auc:.4f} | AUC PR: {aupr:.4f} | Acc: {accuracy:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    return pred, {\"auc_roc\": auc, \"auc_pr\": aupr, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "models, metrics, predictions = {}, {}, {}\n",
    "for name in [\"lr\",\"rf\",\"gbt\"]:\n",
    "    print(f\"Entrenando modelo: {name}\")\n",
    "    m = build_and_fit(name, train)\n",
    "    pred, mtx = evaluate_model(m, test, model_name=name)\n",
    "    models[name] = m; metrics[name] = mtx; predictions[name] = pred.select(\"cliente_id\",\"ym\",\"label\",\"p_digital\")\n",
    "\n",
    "best_name = sorted(metrics.items(), key=lambda x: x[1][\"auc_pr\"], reverse=True)[0][0]\n",
    "best_model = models[best_name]\n",
    "best_pred  = predictions[best_name]\n",
    "print(\"\\n>>> Mejor modelo por AUC PR:\", best_name, metrics[best_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063baf02",
   "metadata": {},
   "source": [
    "## 7) Tuning de umbral (max F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_with_threshold_from_pred(pred_df, thr: float):\n",
    "    p = pred_df.withColumn(\"pred_thr\", (F.col(\"p_digital\") >= F.lit(thr)).cast(\"int\"))\n",
    "    agg = p.agg(\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"tn\"),\n",
    "        F.sum(F.when((F.col(\"label\")==0) & (F.col(\"pred_thr\")==1), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"label\")==1) & (F.col(\"pred_thr\")==0), 1).otherwise(0)).alias(\"fn\")\n",
    "    ).first()\n",
    "    tp, tn, fp, fn = [float(agg[x] or 0.0) for x in (\"tp\",\"tn\",\"fp\",\"fn\")]\n",
    "    total = tp + tn + fp + fn or 1.0\n",
    "    accuracy  = (tp + tn) / total\n",
    "    precision = tp / (tp + fp or 1.0)\n",
    "    recall    = tp / (tp + fn or 1.0)\n",
    "    f1        = (2 * precision * recall) / (precision + recall or 1e-9)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "grid = [x/100 for x in range(30, 81, 5)]\n",
    "rows = [(t,)+metrics_with_threshold_from_pred(best_pred, t) for t in grid]\n",
    "thr_df = spark.createDataFrame(rows, [\"threshold\",\"accuracy\",\"precision\",\"recall\",\"f1\"]).orderBy(F.desc(\"f1\"))\n",
    "thr_df.show(20, False)\n",
    "\n",
    "best_thr = thr_df.first()[\"threshold\"]\n",
    "acc, pre, rec, f1 = metrics_with_threshold_from_pred(best_pred, best_thr)\n",
    "print(f\"Mejor threshold: {best_thr:.2f} | Acc: {acc:.4f}  Prec: {pre:.4f}  Rec: {rec:.4f}  F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2da59c",
   "metadata": {},
   "source": [
    "## 8) Exportar artefactos y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39946864",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{best_name}_next_digital_v3_gpu\"\n",
    "best_model.bestModel.write().overwrite().save(models_dir)\n",
    "\n",
    "from datetime import datetime\n",
    "metrics_rows = [\n",
    "    (\"model\", best_name),\n",
    "    (\"auc_pr\", float(metrics[best_name][\"auc_pr\"])),\n",
    "    (\"auc_roc\", float(metrics[best_name][\"auc_roc\"])),\n",
    "    (\"accuracy\", float(metrics[best_name][\"accuracy\"])),\n",
    "    (\"precision\", float(metrics[best_name][\"precision\"])),\n",
    "    (\"recall\", float(metrics[best_name][\"recall\"])),\n",
    "    (\"f1\", float(metrics[best_name][\"f1\"])),\n",
    "    (\"best_threshold\", float(best_thr)),\n",
    "    (\"generated_at\", datetime.utcnow().isoformat())\n",
    "]\n",
    "spark.createDataFrame([(k, v) for k, v in metrics_rows], [\"metric\",\"value\"]).coalesce(1)      .write.mode(\"overwrite\").json(\"results/metrics_v3_gpu\")\n",
    "\n",
    "best_pred.write.mode(\"overwrite\").parquet(\"results/pred_test_v3_gpu\")\n",
    "\n",
    "print(\"Modelo guardado en:\", models_dir)\n",
    "print(\"Métricas y predicciones guardadas en 'results/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067672b",
   "metadata": {},
   "source": [
    "## 9) Cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
